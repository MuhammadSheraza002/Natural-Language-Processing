{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bbc90c-96a6-41f8-a1b7-d5520d4ca6c5",
   "metadata": {},
   "source": [
    "---   \n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Natural Language Processing</h1>\n",
    "\n",
    "--- \n",
    "<h2><div align=\"right\">Muhammad Sheraz (Data Scientist)</div></h2>\n",
    "<h1 align=\"center\">Lecture 4: (Text Representation)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a7c01-e81c-4b2f-893b-6bc8549a086c",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"1100\"  src=\"../images/phase3.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37206d8-a956-4ec9-b4cf-642fdac9cabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7401bed6-feec-43c6-bce6-c06a7cc9760e",
   "metadata": {},
   "source": [
    "## Learning agenda of this notebook\n",
    "\n",
    "1. **Feature Extraction**\n",
    "    - Bag of Words (BoW) Representation\n",
    "    - Creating Bag of N-grams\n",
    "    - Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7be4b9-d04f-4e4e-ba11-3d6e2c821c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1437b-235d-436d-8c8b-3931578bd0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29413ede-e7d7-474b-bbaf-d7f83552da69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7fc8b-695f-4fe1-b5fb-587940c6317f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267eed77-1a28-45a0-8737-553fa5961df3",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) Representation\r\n",
    "\r\n",
    "- **Definition:**\r\n",
    "  - BoW is a common technique in natural language processing (NLP) to represent text data as a numerical matrix.\r\n",
    "  - It focuses on the occurrence and frequency of words in a document, disregarding grammar and word order.\r\n",
    "\r\n",
    "- **Process:**\r\n",
    "  - **Tokenization:** Breaks text into individual words or tokens.\r\n",
    "  - **Lowercasing:** Converts all words to lowercase to ensure uniformity.\r\n",
    "  - **Stopword Removal:** Eliminates common words (e.g., 'the', 'is') that add little meaning.\r\n",
    "  - **Counting:** Creates a matrix where each row represents a document, and each column represents a unique word, counting the occurrences.\r\n",
    "\r\n",
    "- **Key Components:**\r\n",
    "  - **Document-Term Matrix (DTM):** The resulting matrix showing the frequency of each word in each document.\r\n",
    "  - **Vocabulary:** The set of unique words across all documents.\r\n",
    "\r\n",
    "- **Pr,initutiveos:**\r\n",
    "  - Simple and computationally efficient.\r\n",
    "  - Captures important term frequencies for basic text anal\n",
    "  - Sparcity ysis.\r\n",
    "\r\n",
    "- **Cons:**\r\n",
    "  - Ignores word order and semantics.\r\n",
    "  - Doesn't consider relationships between words (e.g., 'good' and 'great' are treated as separate entities).\r\n",
    "\r\n",
    "- **Use Cases:**\r\n",
    "  - Commonly used in text classification, sentiment analysis, and information retrieval.\r\n",
    "  - Foundation for more advanced NLP techniques like TF-IDF and word embeddings.\r\n",
    "\r\n",
    "- **Libraries:**\r\n",
    "  - Popular libraries like scikit-learn in Python provide tools (e.g., `CountVectorizer`) for easy implementation.\r\n",
    "\r\n",
    "- **Considerations:**\r\n",
    "  - Customize preprocessing steps and hyperparameters based on specific needs.\r\n",
    "  - May require additional techniques for handling large vocabularies or improving semantic understanding.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6660906-7881-4316-bc94-8385d69bff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe1595c-dd14-4b97-b8a7-d789b0c01ad1",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) Example\n",
    "\n",
    "Consider a corpus with three documents:\n",
    "\n",
    "1. Document 1: \"This is the first document.\"\n",
    "2. Document 2: \"This document is the second document.\"\n",
    "3. Document 3: \"And this is the third one.\"\n",
    "\n",
    "#### Step 1: Tokenization\n",
    "\n",
    "- Unique words across all documents:\n",
    "  - {This, is, the, first, document, second, And, third, one}\n",
    "\n",
    "#### Step 2: Create Vocabulary\n",
    "\n",
    "- Assign an index to each unique word:\n",
    "  - Vocabulary: {This: 0, is: 1, the: 2, first: 3, document: 4, second: 5, And: 6, third: 7, one: 8}\n",
    "\n",
    "#### Step 3: Document-Term Matrix (DTM)\n",
    "\n",
    "- Represent each document as a vector of word frequencies based on the vocabulary:\n",
    "\n",
    "| Document | This | is | the | first | document | second | And | third | one |\n",
    "|----------|------|----|-----|-------|----------|--------|-----|-------|-----|\n",
    "| 1        | 1    | 1  | 1   | 1     | 1        | 0      | 0   | 0     | 0   |\n",
    "| 2        | 1    | 1  | 1   | 0     | 2        | 1      | 0   | 0     | 0   |\n",
    "| 3        | 1    | 1  | 1   | 0     | 0        | 0      | 1   | 1     | 1   |\n",
    "\n",
    "The Document-Term Matrix (DTM) captures the frequency of each word in each document, forming the basis of the Bag of Words (BoW) representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e85a6-e56b-4d8a-a48d-a294b1137c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110edaab-cc2b-463e-ac13-d06403494632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa246e3-8dc5-45a3-b8c0-4ad88c570649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This document is the second document.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the t||hird one.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text  Output\n",
       "0            This is the first document.       1\n",
       "1  This document is the second document.       0\n",
       "2           And this is the t||hird one.       1\n",
       "3            Is this the first document?       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {'Text': ['This is the first document.',\n",
    "                 'This document is the second document.',\n",
    "                 'And this is the t||hird one.',\n",
    "                 'Is this the first document?'],\n",
    "        'Output': [1, 0, 1, 0]}  # Adding a binary output column\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e3ef6-aeb3-41ce-8955-5bd08d99d6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6df5eaa-996f-4b50-a6af-a188ae483fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>hird</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  hird  is  one  second  the  this\n",
       "0    0         1      1     0   1    0       0    1     1\n",
       "1    0         2      0     0   1    0       1    1     1\n",
       "2    1         0      0     1   1    1       0    1     1\n",
       "3    0         1      1     0   1    0       0    1     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply bag-of-words representation with specified hyperparameters\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,      # Convert all characters to lowercase\n",
    "    #stop_words='english', # Remove common English stop words\n",
    "    max_features=None,    # Keep all unique words (no limit on features)\n",
    "    binary=False,         # Count occurrences (binary=False) or presence (binary=True)\n",
    "    ngram_range=(1, 1)    # Use unigrams (single words), can be adjusted for bigrams, trigrams, etc.\n",
    ")\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Convert the bag-of-words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0097d53c-3103-439f-a1ca-9eff84f8e109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 2, 0, 0, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 1, 1],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb26421-aa95-4050-97df-a9e1565e4372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbbd621-5696-4e38-80e0-8c8d637c89c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix[3].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646f44db-e991-4448-96a5-b4ae6c2283d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['document is the final document']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bb593-0b27-43b0-ac07-baf87bcb9968",
   "metadata": {},
   "source": [
    "## Understanding `vectorizer.vocabulary_` in `CountVectorizer`\n",
    "\n",
    "In the scikit-learn `CountVectorizer` class, `vectorizer.vocabulary_` is an attribute that provides a mapping between terms (words) and their indices in the bag-of-words matrix. Here's how it works:\n",
    "\n",
    "### Vocabulary Building Process\n",
    "\n",
    "1. **Tokenization:**\n",
    "   - The text data is tokenized, breaking it into individual words or tokens.\n",
    "\n",
    "2. **Lowercasing:**\n",
    "   - All words are converted to lowercase to ensure consistency.\n",
    "\n",
    "3. **Stopword Removal (if specified):**\n",
    "   - Common English stop words (e.g., 'the', 'is', 'and') may be removed based on the `stop_words` parameter.\n",
    "\n",
    "4. **Building the Vocabulary:**\n",
    "   - For each unique word in the preprocessed text data, a unique index is assigned.\n",
    "   - The resulting vocabulary is stored in a dictionary, where the keys are the words, and the values are their corresponding indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f7a09-64f6-44e2-8cc4-ce6d8b6c10b1",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "Consider the following text data:\n",
    "```plaintext\n",
    "['This is the first document.',\n",
    " 'This document is the second document.',\n",
    " 'And this is the third one.',\n",
    " 'Is this the first document?']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a03f4d-aade-4ab7-bce0-d1622e9c4710",
   "metadata": {},
   "source": [
    "### Applying the tokenization, lowercasing, and stopword removal processes, we get a set of unique words:\n",
    "\n",
    "{'document', 'second', 'one', 'third'}\r",
    "### The vocabulary dictionary would look like:\n",
    "{\r\n",
    " 'document': 0,\r\n",
    " 'second': 1,\r\n",
    " 'one': 2,\r\n",
    " 'third': 3\r\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb564a-f144-42be-a6fd-3f6d89f935fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a420b8-233d-49d7-bda1-40111b4989a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784a7a70-8343-4f5a-99fa-637fd9a380fc",
   "metadata": {},
   "source": [
    "## Example of Vocabulary Building in Bag-of-Words (BoW)\n",
    "\n",
    "Consider the following text data:\n",
    "\n",
    "1. \"This is the first document.\"\n",
    "2. \"This document is the second document.\"\n",
    "3. \"And this is the third one.\"\n",
    "4. \"Is this the first document?\"\n",
    "\n",
    "### Step 1: Tokenization\n",
    "\n",
    "Tokenize the text into individual words:\n",
    "\n",
    "[\"This\", \"is\", \"the\", \"first\", \"document\", \"This\", \"document\", \"is\", \"the\", \"second\", \"document\", \"And\", \"this\", \"is\", \"the\", \"third\", \"one\", \"Is\", \"this\", \"the\", \"first\", \"document\"]\n",
    "\n",
    "### Step 2: Lowercasing\r\n",
    "\r\n",
    "Convert all words to lowercase:\r\n",
    "\r\n",
    "[\"this\", \"is\", \"the\", \"first\", \"document\", \"this\", \"document\", \"is\", \"the\", \"second\", \"document\", \"and\", \"this\", \"is\", \"the\", \"third\", \"one\", \"is\", \"this\", \"the\", \"first\", \"docume#\n",
    "\n",
    "\r\n",
    "## Step 3: Vocabulary Building\r\n",
    "\r\n",
    "Build the vocabulary by assigning unique indices to each unique word:\r\n",
    "\r\n",
    "```markdown\r\n",
    "{\r\n",
    " 'this': 0,\r\n",
    " 'is': 1,\r\n",
    " 'the': 2,\r\n",
    " 'first': 3,\r\n",
    " 'document': 4,\r\n",
    " 'second': 5,\r\n",
    " 'and': 6,\r\n",
    " 'third': 7,\r\n",
    " 'one': 8,\r\n",
    " 'second': 9\r\n",
    "}\r\n",
    "nt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d29932-22d2-4948-9884-7e25f52146cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e22b9-1526-4004-a903-58ae674707d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b4969-6222-4712-82dd-3df4df0b0376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f8760-3e90-4c52-ba74-c65cb186c3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169dc595-7962-4a81-8cf3-5b9fdddebcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 4,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'hird': 3,\n",
       " 'one': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8155c-42a8-476c-8eb1-21d15847ba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cfb45-c951-4aa8-8d89-448fc4fb4a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be15c3e2-8ff3-421d-b265-639c8c4e3059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Output</th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>hird</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This document is the second document.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the t||hird one.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text  Output  and  document  first  hird  \\\n",
       "0            This is the first document.       1    0         1      1     0   \n",
       "1  This document is the second document.       0    0         2      0     0   \n",
       "2           And this is the t||hird one.       1    1         0      0     1   \n",
       "3            Is this the first document?       0    0         1      1     0   \n",
       "\n",
       "   is  one  second  the  this  \n",
       "0   1    0       0    1     1  \n",
       "1   1    0       1    1     1  \n",
       "2   1    1       0    1     1  \n",
       "3   1    0       0    1     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.concat([df, bow_df], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea7d73-6a27-4af8-9867-453fce07573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e27892-d636-43c2-8cc7-069926003a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d7d87-398b-4058-afdc-344f76d16f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3bdc-165b-4854-9047-3d7567ac6a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f87838ca-09d2-4317-be82-4f8adac46c69",
   "metadata": {},
   "source": [
    "## CountVectorizer Hyperparameters\r\n",
    "\r\n",
    "- **`lowercase` (default=True):**\r\n",
    "  - *Description:* Converts all text to lowercase. Helps in treating words with different cases as the same.\r\n",
    "  - *Default Value:* True\r\n",
    "  - *Use Case:* Set to False if you want to preserve the case sensitivity of words.\r\n",
    "\r\n",
    "- **`stop_words` (default=None):**\r\n",
    "  - *Description:* Removes common English stop words (e.g., 'the', 'is', 'and') to focus on more meaningful words.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Pass 'english' to remove common English stop words. Custom stop words can be provided as a list.\r\n",
    "\r\n",
    "- **`max_features` (default=None):**\r\n",
    "  - *Description:* Limits the number of unique words to consider. If specified, the most frequent words are selected.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Set a specific number to limit the vocabulary size, helpful when dealing with large datasets or when focusing on top words.\r\n",
    "\r\n",
    "- **`binary` (default=False):**\r\n",
    "  - *Description:* If True, the matrix representation is binary (1 if the word is present, 0 if not). If False, it counts the occurrences.\r\n",
    "  - *Default Value:* False\r\n",
    "  - *Use Case:* Set to True for binary representation when only presence/absence matters, not the frequency.\r\n",
    "\r\n",
    "- **`ngram_range` (default=(1, 1)):**\r\n",
    "  - *Description:* Specifies the range of n-grams to consider. For example, (1, 1) considers only unigrams, (1, 2) considers unigrams and bigrams, etc.\r\n",
    "  - *Default Value:* (1, 1)\r\n",
    "  - *Use Case:* Adjust to capture more context by including bigrams (or trigrams) in addition to unigrams.\r\n",
    "\r\n",
    "- **`tokenizer` (default=None):**\r\n",
    "  - *Description:* Custom function for tokenization. If None, it uses the default tokenizer.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Provide a custom tokenizer function if the default tokenization is not suitable for your data.\r\n",
    "\r\n",
    "- **`preprocessor` (default=None):**\r\n",
    "  - *Description:* Custom function applied to each document before tokenization and stop word removal.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Use when additional preprocessing is required before tokenization.\r\n",
    "\r\n",
    "- **`max_df` (default=1.0):**\r\n",
    "  - *Description:* Ignores terms that have a document frequency strictly higher than the specified threshold (float or integer).\r\n",
    "  - *Default Value:* 1.0\r\n",
    "  - *Use Case:* Exclude words that are too common and may not provide meaningful information.\r\n",
    "\r\n",
    "- **`min_df` (default=1):**\r\n",
    "  - *Description:* Ignores terms that have a document frequency strictly lower than the specified threshold (float or integer).\r\n",
    "  - *Default Value:* 1\r\n",
    "  - *Use Case:* Exclude words that are too rare and may not contribute much to the analysis.\r\n",
    "\r\n",
    "- **`vocabulary` (default=None):**\r\n",
    "  - *Description:* List of words to consider. If not None, it ignores all terms that are not in this list.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Provide a custom vocabulary list to restrict the features to a predefined set.\r\n",
    "\r\n",
    "- **`strip_accents` (default=None):**\r\n",
    "  - *Description:* Remove accents during the preprocessing step.\r\n",
    "  - *Default Value:* None\r\n",
    "  - *Use Case:* Set to 'ascii' or 'unicode' to remove accents from words.\r\n",
    "\r\n",
    "- **`token_pattern` (default=r\"(?u)\\b\\w\\w+\\b\"):**\r\n",
    "  - *Description:* Regular expression defining what constitutes a 'word' and how to split it.\r\n",
    "  - *Default Value:* r\"(?u)\\b\\w\\w+\\b\"\r\n",
    "  - *Use Case:* Customize the pattern to suit specific tokenization requirements.\r\n",
    "\r\n",
    "- **`analyzer` (default='word'):**\r\n",
    "  - *Description:* Determines whether the feature should be made of word n-gram or character n-grams.\r\n",
    "  - *Default Value:* 'word'\r\n",
    "  - *Use Case:* Set to 'char' or 'char_wb' for character n-grams instead of word n-grams.\r\n",
    "\r\n",
    "- **`dtype` (default=np.int64):**\r\n",
    "  - *Description:* Type of the matrix returned.\r\n",
    "  - *Default Value:* np.int64\r\n",
    "  - *Use Case:* Adjust if a different data type for the matrix is required.\r\n",
    "\r\n",
    "- **`input` (default='content'):**\r\n",
    "  - *Description:* 'content' interprets input as a collection of raw text documents.\r\n",
    "  - *Default Value:* 'content'\r\n",
    "  - *Use Case:* Typically, there's no need to change this unless the input format is different.\r\n",
    "\r\n",
    "These hyperparameters offer flexibility in customizing the behavior of the `CountVectorizer` based on specific requirements and characteristics of the input text data. Adjusting these hyperparameters allows for fine-tuning the Bag of Words representation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590c8e9-9307-4261-978b-b832b790b682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58115cb8-5031-4e57-82e1-7bf26c60e682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed2f4c3d-58c9-4ed1-8cb6-648d4f3fdc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first book about a thrilling adven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Book2 is a non-fiction work by AuthorB.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The third book is a fictional story by AuthorA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AuthorC wrote a mystery novel in Book4.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  This is the first book about a thrilling adven...\n",
       "1            Book2 is a non-fiction work by AuthorB.\n",
       "2    The third book is a fictional story by AuthorA.\n",
       "3            AuthorC wrote a mystery novel in Book4."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an advanced sample dataframe\n",
    "data = {\n",
    "        'Text': ['This is the first book about a thrilling adventure.',\n",
    "                 'Book2 is a non-fiction work by AuthorB.',\n",
    "                 'The third book is a fictional story by AuthorA.',\n",
    "                 'AuthorC wrote a mystery novel in Book4.']}\n",
    "df_advanced = pd.DataFrame(data)\n",
    "\n",
    "# Display the advanced sample dataframe\n",
    "df_advanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d032e083-3b2e-4a0f-92fa-463158e2257a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adventure</th>\n",
       "      <th>authora</th>\n",
       "      <th>authorb</th>\n",
       "      <th>authorc</th>\n",
       "      <th>book</th>\n",
       "      <th>book2</th>\n",
       "      <th>book4</th>\n",
       "      <th>fiction</th>\n",
       "      <th>fictional</th>\n",
       "      <th>mystery</th>\n",
       "      <th>non</th>\n",
       "      <th>novel</th>\n",
       "      <th>story</th>\n",
       "      <th>thrilling</th>\n",
       "      <th>work</th>\n",
       "      <th>wrote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adventure  authora  authorb  authorc  book  book2  book4  fiction  \\\n",
       "0          1        0        0        0     1      0      0        0   \n",
       "1          0        0        1        0     0      1      0        1   \n",
       "2          0        1        0        0     1      0      0        0   \n",
       "3          0        0        0        1     0      0      1        0   \n",
       "\n",
       "   fictional  mystery  non  novel  story  thrilling  work  wrote  \n",
       "0          0        0    0      0      0          1     0      0  \n",
       "1          0        0    1      0      0          0     1      0  \n",
       "2          1        0    0      0      1          0     0      0  \n",
       "3          0        1    0      1      0          0     0      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply bag-of-words representation\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(df_advanced['Text'])\n",
    "\n",
    "# Convert the bag-of-words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c33870-65ef-46e0-b024-f3a1e10fe0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 4,\n",
       " 'thrilling': 13,\n",
       " 'adventure': 0,\n",
       " 'book2': 5,\n",
       " 'non': 10,\n",
       " 'fiction': 7,\n",
       " 'work': 14,\n",
       " 'authorb': 2,\n",
       " 'fictional': 8,\n",
       " 'story': 12,\n",
       " 'authora': 1,\n",
       " 'authorc': 3,\n",
       " 'wrote': 15,\n",
       " 'mystery': 9,\n",
       " 'novel': 11,\n",
       " 'book4': 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6bb8f-ee7e-46e6-880c-c8edefe04c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103df1ea-2df0-4d46-9de6-1b7be1982027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>adventure</th>\n",
       "      <th>authora</th>\n",
       "      <th>authorb</th>\n",
       "      <th>authorc</th>\n",
       "      <th>book</th>\n",
       "      <th>book2</th>\n",
       "      <th>book4</th>\n",
       "      <th>fiction</th>\n",
       "      <th>fictional</th>\n",
       "      <th>mystery</th>\n",
       "      <th>non</th>\n",
       "      <th>novel</th>\n",
       "      <th>story</th>\n",
       "      <th>thrilling</th>\n",
       "      <th>work</th>\n",
       "      <th>wrote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first book about a thrilling adven...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Book2 is a non-fiction work by AuthorB.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The third book is a fictional story by AuthorA.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AuthorC wrote a mystery novel in Book4.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  adventure  authora  \\\n",
       "0  This is the first book about a thrilling adven...          1        0   \n",
       "1            Book2 is a non-fiction work by AuthorB.          0        0   \n",
       "2    The third book is a fictional story by AuthorA.          0        1   \n",
       "3            AuthorC wrote a mystery novel in Book4.          0        0   \n",
       "\n",
       "   authorb  authorc  book  book2  book4  fiction  fictional  mystery  non  \\\n",
       "0        0        0     1      0      0        0          0        0    0   \n",
       "1        1        0     0      1      0        1          0        0    1   \n",
       "2        0        0     1      0      0        0          1        0    0   \n",
       "3        0        1     0      0      1        0          0        1    0   \n",
       "\n",
       "   novel  story  thrilling  work  wrote  \n",
       "0      0      0          1     0      0  \n",
       "1      0      0          0     1      0  \n",
       "2      0      1          0     0      0  \n",
       "3      1      0          0     0      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_advanced = pd.concat([df_advanced, bow_df], axis=1)\n",
    "\n",
    "result_df_advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b6bd4-4f5e-4647-b8fc-d0def4cded3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c43df1-06ad-43b8-8887-b7471391ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bb8dc-770d-40a4-b8b0-ad2867fabc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb8e56c-9984-42d4-af83-667a5797d3c6",
   "metadata": {},
   "source": [
    "## g. Creating N-grams\n",
    "- **What are n-grams?** \n",
    "    - A sequence of n words, can be bigram, trigram,....\n",
    "- **Why to use n-grams?** \n",
    "    - Capture contextual information (`good food` carries more meaning than just `good` and `food` when observed independently)\n",
    "    - Applications of N-grams:\n",
    "        - Sentence Completion\n",
    "        - Auto Spell Check and correction\n",
    "        - Auto Grammer Check and correction\n",
    "    - Is there a perfect value of n?\n",
    "        - Different types of n-grams are suitable for different types of applications. You should try different n-grams on your data in order to confidently conclude which one works the best among all for your text analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3d32b-7b9d-419c-99fe-4d0df2ea7c47",
   "metadata": {},
   "source": [
    "- **How to create n-grams?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63f28689-47e1-4c10-aab4-5200a48a72ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object bigrams at 0x0000023602AAEAB0>\n",
      "('Allama', 'Iqbal')\n",
      "('Iqbal', 'was')\n",
      "('was', 'a')\n",
      "('a', 'visionary')\n",
      "('visionary', 'philosopher')\n",
      "('philosopher', 'and')\n",
      "('and', 'politician')\n",
      "('politician', '.')\n",
      "('.', 'Thank')\n",
      "('Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mystr = \"Allama Iqbal was a visionary philosopher and politician. Thank you\"\n",
    "tokens = nltk.tokenize.word_tokenize(mystr)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "print(bgs)\n",
    "for grams in bgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c6b2d-c2b7-43ea-a079-975c50b9e61a",
   "metadata": {},
   "source": [
    ">- The formula to calculate the count of n-grams in a document is: **`X - N + 1`**, where `X` is the number of words in a given document and `N` is the number of words in n-gram\n",
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 2 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 10\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15223d51-ec8a-439e-9afa-cc7713800dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was')\n",
      "('Iqbal', 'was', 'a')\n",
      "('was', 'a', 'visionary')\n",
      "('a', 'visionary', 'philosopher')\n",
      "('visionary', 'philosopher', 'and')\n",
      "('philosopher', 'and', 'politician')\n",
      "('and', 'politician', '.')\n",
      "('politician', '.', 'Thank')\n",
      "('.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "tgs = nltk.trigrams(tokens)\n",
    "for grams in tgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0e7f5-6feb-4762-bb05-a300605598ae",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 3 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 9\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ea5a6b2-4e14-4c73-9422-ce33021fc298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was', 'a')\n",
      "('Iqbal', 'was', 'a', 'visionary')\n",
      "('was', 'a', 'visionary', 'philosopher')\n",
      "('a', 'visionary', 'philosopher', 'and')\n",
      "('visionary', 'philosopher', 'and', 'politician')\n",
      "('philosopher', 'and', 'politician', '.')\n",
      "('and', 'politician', '.', 'Thank')\n",
      "('politician', '.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "ngrams = nltk.ngrams(tokens, 4)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24d9c6-cf43-49d5-ae9c-249cae01a3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3b793-453a-4ada-a445-7df00a0bbc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eba9c09-3525-417e-8981-634666f7e1ab",
   "metadata": {},
   "source": [
    "## N-Grams in Bag of Words (BoW)\n",
    "\n",
    "- **Definition:**\n",
    "  - N-Grams extend the BoW representation by considering sequences of 'n' consecutive words as a single feature.\n",
    "  - Unigrams (1-grams) are single words, bigrams (2-grams) are pairs of consecutive words, trigrams (3-grams) are triplets, and so on.\n",
    "\n",
    "- **Enhancements:**\n",
    "  - **Contextual Information:** Captures the context and relationship between adjacent words.\n",
    "  - **Increased Complexity:** Larger 'n' introduces more features, leading to a richer representation.\n",
    "\n",
    "- **Use Cases:**\n",
    "  - Useful in tasks where word order matters, such as language modeling and certain types of text analysis.\n",
    "  - Provides more nuanced information for understanding the meaning of phrases.\n",
    "\n",
    "- **Implementation:**\n",
    "  - Supported in libraries like scikit-learn through the `ngram_range` parameter in `CountVectorizer`.\n",
    "\n",
    "- **Considerations:**\n",
    "  - Larger 'n' increases the dimensionality of the feature space, which may impact computational efficiency.\n",
    "  - Finding the right balance between granularity and complexity is crucial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d625b5-7212-4066-be58-a9a5cfd45013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ab65b5-b1b2-4a23-8bbe-98943ceaf63c",
   "metadata": {},
   "source": [
    "## N-Grams in Bag of Words (BoW) Example\n",
    "\n",
    "Consider a corpus with three documents:\n",
    "\n",
    "1. Document 1: \"This is the first document.\"\n",
    "2. Document 2: \"This document is the second document.\"\n",
    "3. Document 3: \"And this is the third one.\"\n",
    "\n",
    "#### Step 1: Tokenization and N-Gram Formation\n",
    "\n",
    "- Unique unigrams and bigrams across all documents:\n",
    "  - {This, is, the, first, document, second, And, third, one, This is, is the, the first, first document, document This, is the, the second, second document, And this, this is, is the, the third, third one}\n",
    "\n",
    "#### Step 2: Create Vocabulary\n",
    "\n",
    "- Assign an index to each unique n-gram:\n",
    "  - Vocabulary: {This: 0, is: 1, the: 2, first: 3, document: 4, second: 5, And: 6, third: 7, one: 8, This is: 9, is the: 10, the first: 11, first document: 12, document This: 13, is the: 14, the second: 15, second document: 16, And this: 17, this is: 18, is the: 19, the third: 20, third one: 21}\n",
    "\n",
    "#### Step 3: Document-Term Matrix (DTM) for N-Grams\n",
    "\n",
    "- Represent each document as a vector of n-gram frequencies based on the vocabulary:\n",
    "\n",
    "| Document | This | is | the | first | document | second | And | third | one | This is | is the | the first | first document | document This | is the | the second | second document | And this | this is | is the | the third | third one |\n",
    "|----------|------|----|-----|-------|----------|--------|-----|-------|-----|---------|--------|------------|-----------------|----------------|--------|-------------|------------------|-----------|---------|--------|-----------|-----------|\n",
    "| 1        | 1    | 1  | 1   | 1     | 1        | 0      | 0   | 0     | 0   | 1       | 1      | 1          | 1               | 1              | 0      | 0           | 0                | 0         | 0       | 0      | 0         | 0         |\n",
    "| 2        | 1    | 1  | 1   | 0     | 2        | 1      | 0   | 0     | 0   | 1       | 1      | 0          | 0               | 0              | 1      | 1           | 1                | 0         | 0       | 0      | 0         | 0         |\n",
    "| 3        | 1    | 1  | 1   | 0     | 0        | 0      | 1   | 1     | 1   | 0       | 0      | 0          | 0               | 0              | 1      | 0           | 0                | 1         | 1       | 1      | 1         | 1         |\n",
    "\n",
    "The Document-Term Matrix (DTM) for N-Grams captures the frequency of each n-gram in each document, forming the basis of the Bag of Words (BoW) representation with N-Grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235da910-a58d-44e4-8181-59fd077da347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11728ab2-7d2f-4254-bac9-6573c42a9a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This document is the second document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the third one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text\n",
       "0            This is the first document.\n",
       "1  This document is the second document.\n",
       "2             And this is the third one.\n",
       "3            Is this the first document?"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {'Text': ['This is the first document.',\n",
    "                 'This document is the second document.',\n",
    "                 'And this is the third one.',\n",
    "                 'Is this the first document?']}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02617bc1-0746-4015-adce-f862654c317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>and</th>\n",
       "      <th>and this</th>\n",
       "      <th>and this is</th>\n",
       "      <th>document</th>\n",
       "      <th>document is</th>\n",
       "      <th>document is the</th>\n",
       "      <th>first</th>\n",
       "      <th>first document</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>the third one</th>\n",
       "      <th>third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this</th>\n",
       "      <th>this document</th>\n",
       "      <th>this document is</th>\n",
       "      <th>this is</th>\n",
       "      <th>this is the</th>\n",
       "      <th>this the</th>\n",
       "      <th>this the first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This document is the second document.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the third one.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text  and  and this  and this is  \\\n",
       "0            This is the first document.    0         0            0   \n",
       "1  This document is the second document.    0         0            0   \n",
       "2             And this is the third one.    1         1            1   \n",
       "3            Is this the first document?    0         0            0   \n",
       "\n",
       "   document  document is  document is the  first  first document  is  ...  \\\n",
       "0         1            0                0      1               1   1  ...   \n",
       "1         2            1                1      0               0   1  ...   \n",
       "2         0            0                0      0               0   1  ...   \n",
       "3         1            0                0      1               1   1  ...   \n",
       "\n",
       "   the third one  third  third one  this  this document  this document is  \\\n",
       "0              0      0          0     1              0                 0   \n",
       "1              0      0          0     1              1                 1   \n",
       "2              1      1          1     1              0                 0   \n",
       "3              0      0          0     1              0                 0   \n",
       "\n",
       "   this is  this is the  this the  this the first  \n",
       "0        1            1         0               0  \n",
       "1        0            0         0               0  \n",
       "2        1            1         0               0  \n",
       "3        0            0         1               1  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply N-Grams using CountVectorizer\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))  # This specifies using unigrams and bigrams\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Convert the N-Grams matrix to a DataFrame\n",
    "ngram_df = pd.DataFrame(ngram_matrix.toarray(), columns=ngram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the N-Grams DataFrame with the original dataframe\n",
    "result_df = pd.concat([df, ngram_df], axis=1)\n",
    "\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1eb23d29-dbaf-4ef1-891a-37eabf3ae2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 27,\n",
       " 'is': 8,\n",
       " 'the': 18,\n",
       " 'first': 6,\n",
       " 'document': 3,\n",
       " 'this is': 30,\n",
       " 'is the': 9,\n",
       " 'the first': 19,\n",
       " 'first document': 7,\n",
       " 'this is the': 31,\n",
       " 'is the first': 10,\n",
       " 'the first document': 20,\n",
       " 'second': 16,\n",
       " 'this document': 28,\n",
       " 'document is': 4,\n",
       " 'the second': 21,\n",
       " 'second document': 17,\n",
       " 'this document is': 29,\n",
       " 'document is the': 5,\n",
       " 'is the second': 11,\n",
       " 'the second document': 22,\n",
       " 'and': 0,\n",
       " 'third': 25,\n",
       " 'one': 15,\n",
       " 'and this': 1,\n",
       " 'the third': 23,\n",
       " 'third one': 26,\n",
       " 'and this is': 2,\n",
       " 'is the third': 12,\n",
       " 'the third one': 24,\n",
       " 'is this': 13,\n",
       " 'this the': 32,\n",
       " 'is this the': 14,\n",
       " 'this the first': 33}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333032a-1c95-4fcf-95d3-c3a9e472132c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11786628-7ed0-417c-b33c-1d783e1fab37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a23751-89d7-4cc4-8426-9b8a4121b682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0e06a-5d72-4d13-80b2-d867b35a42d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0bace-c462-456d-84a7-c27b26a329b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd0fdb2-e279-4dee-a9bd-759a2a48f92c",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)\r\n",
    "\r\n",
    "- **Definition:**\r\n",
    "  - TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents.\r\n",
    "  - It balances the frequency of a word in a document (Term Frequency - TF) with its rarity across documents (Inverse Document Frequency - IDF).\r\n",
    "\r\n",
    "- **Components:**\r\n",
    "  - **Term Frequency (TF):**\r\n",
    "    - Measures how often a term appears in a document.\r\n",
    "    - Calculated as the number of occurrences of a term divided by the total number of terms in the document.\r\n",
    "\r\n",
    "  - **Inverse Document Frequency (IDF):**\r\n",
    "    - Measures how unique or rare a term is across all documents.\r\n",
    "    - Calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\r\n",
    "\r\n",
    "- **Formula:**\r\n",
    "  - TF-IDF = TF * IDF\r\n",
    "\r\n",
    "- **Key Characteristics:**\r\n",
    "  - Words with high TF-IDF scores are important in a specific document but not common across all documents.\r\n",
    "  - Common words receive lower scores, emphasizing the uniqueness of terms.\r\n",
    "\r\n",
    "- **Pros:**\r\n",
    "  - Captures the importance of terms in the context of a document and a corpus.\r\n",
    "  - Penalizes common words and highlights distinctive terms.\r\n",
    "\r\n",
    "- **Cons:**\r\n",
    "  - Complexity increases with a larger corpus.\r\n",
    "  - Requires careful consideration of parameter tuning.\r\n",
    "\r\n",
    "- **Use Cases:**\r\n",
    "  - Document retrieval and ranking.\r\n",
    "  - Keyword extraction.\r\n",
    "  - Text mining and clustering.\r\n",
    "\r\n",
    "- **Libraries:**\r\n",
    "  - Implemented in scikit-leary(), columns=tfidf_vectorizer.get_feature_names_out())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a8a0a-2bf3-4dec-b051-75ba5f99ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8827874d-a866-416d-b5b0-5033f1d16b01",
   "metadata": {},
   "source": [
    "- ### Steps to Calculate TF-IDF\r\n",
    "    \r\n",
    "    - **Step 1: Compute Term Frequency (TF)**\r\n",
    "        - Term Frequency (TF) is calculated for each term in each document. It represents the frequency of a term in a document relative to the total number of terms in that document.\r\n",
    "        \r\n",
    "    - **Step 2: Compute Inverse Document Frequency (IDF)**\r\n",
    "        - Inverse Document Frequency (IDF) measures the importance of a term in the entire corpus. It is computed by taking the logarithm of the ratio of the total number of documents to the number of documents containing the term.\r\n",
    "    \r\n",
    "    - **Step 3: Compute TF-IDF**\r\n",
    "        - TF-IDF (Term Frequency-Inverse Document Frequency) is the product of TF and IDF. It represents the importance of a term in a document relative to its importance in the entire corpus.\r\n",
    "\r\n",
    "The TF-IDF representation captures the importance of terms in each document, emphasizing the uniqueness of terms across the entire corpus.\r\n",
    "s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65337ac-0ef7-4a2c-9af6-570959d40adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63fc00c9-0359-4a2c-9358-23083441e34f",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider a corpus with three documents:\n",
    "\n",
    "1. Document 1: \"This is the first document.\"\n",
    "2. Document 2: \"This document is the second document.\"\n",
    "3. Document 3: \"And this is the third one.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959e6d8-5225-4ded-a254-c3671297dd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c34f7d-e2c7-4d4c-bf3b-c0df965bf487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b316883c-618e-4a82-8b09-e5c3c1326237",
   "metadata": {},
   "source": [
    "### Term Frequency (TF) Table\n",
    "\n",
    "| Term      | Document 1   | Document 2   | Document 3   |\n",
    "|-----------|--------------|--------------|--------------|\n",
    "| This      | 1/5          | 1/5          | 1/6          |\n",
    "| is        | 1/5          | 1/5          | 1/6          |\n",
    "| the       | 1/5          | 1/5          | 1/6          |\n",
    "| first     | 1/5          | 0            | 0            |\n",
    "| document  | 1/5          | 2/5          | 0            |\n",
    "| second    | 0            | 1/5          | 0            |\n",
    "| And       | 0            | 0            | 1/6          |\n",
    "| third     | 0            | 0            | 1/6          |\n",
    "| one       | 0            | 0            | 1/6          |\n",
    "\n",
    "\n",
    "### Inverse Document Frequency (IDF) Table\n",
    "\n",
    "\n",
    "| Term      | IDF          |\n",
    "|-----------|--------------|\n",
    "| This      | 0            |\n",
    "| is        | 0            |\n",
    "| the       | 0            |\n",
    "| first     | 0.477        |\n",
    "| document  | 0.176        |\n",
    "| second    | 0.477        |\n",
    "| And       | 0.477        |\n",
    "| third     | 0.477        |\n",
    "| one       | 0.477        |\n",
    "\n",
    "\n",
    "### TF-IDF Table\n",
    "\n",
    "| Term      | Document 1   | Document 2   | Document 3   |\n",
    "|-----------|--------------|--------------|--------------|\n",
    "| This      | 0            | 0            | 0            |\n",
    "| is        | 0            | 0            | 0            |\n",
    "| the       | 0            | 0            | 0            |\n",
    "| first     | 0.095        | 0            | 0            |\n",
    "| document  | 0.035        | 0.070        | 0            |\n",
    "| second    | 0            | 0.095        | 0            |\n",
    "| And       | 0            | 0            | 0.080        |\n",
    "| third     | 0            | 0            | 0.080        |\n",
    "| one       | 0            | 0            | 0.080        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784867da-daa3-4f1b-afd1-1a132e1c610d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6bd708-aeed-4102-bcaf-675108862d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Display the TF-IDF values in a readable format\n",
    "print(\"TF-IDF Values:\")\n",
    "pd.DataFrame(dense_matrix,columns=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0296db-e0c8-435b-ae31-4eb5cbe5c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Names (Terms):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature names (terms)\n",
    "print(\"\\nFeature Names (Terms):\")\n",
    "feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c6aaa8-9af4-4863-a5f0-765d58410776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                                Document\n",
      "0            This is the first document.\n",
      "1  This document is the second document.\n",
      "2             And this is the third one.\n",
      "\n",
      "TF-IDF DataFrame:\n",
      "       and  document     first        is      one    second       the  \\\n",
      "0  0.00000  0.469417  0.617227  0.364544  0.00000  0.000000  0.364544   \n",
      "1  0.00000  0.728445  0.000000  0.282851  0.00000  0.478909  0.282851   \n",
      "2  0.49712  0.000000  0.000000  0.293607  0.49712  0.000000  0.293607   \n",
      "\n",
      "     third      this  \n",
      "0  0.00000  0.364544  \n",
      "1  0.00000  0.282851  \n",
      "2  0.49712  0.293607  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "data = {\n",
    "    'Document': [\"This is the first document.\",\n",
    "                 \"This document is the second document.\",\n",
    "                 \"And this is the third one.\"]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Document'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Display TF-IDF DataFrame\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0184f-00cd-41bf-9d0d-ba8dfb45459e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d162b058",
   "metadata": {},
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "<h1 align=\"center\">Course: Tools and Techniques for Data Science</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db92d41",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lecture 2 (Basic Text Pre-Processing)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88976106",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/arifpucit/data-science/blob/master/Section-5-(Data-Acquisition)/Lec-5.4(Web-Scraping-using-Selenium-II).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0c47d",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"900\"  src=\"images/phase2.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a6901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c17026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdaacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8ced6b",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "1. **Text Cleaning**\n",
    "    - Removing digits and words containing digits\n",
    "    - Removing newline characters and extra spaces\n",
    "    - Removing HTML tags\n",
    "    - Removing URLs\n",
    "    - Removing punctuations\n",
    "    \n",
    "\n",
    "2. **Basic Text Preprocessing**\n",
    "    - Case folding\n",
    "    - Expand contractions\n",
    "    - Chat word treatment\n",
    "    - Handle emojis\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Creating N-grams\n",
    "    - Stop words Removal\n",
    " \n",
    " \n",
    "3. **Advanced Preprocessing**\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - POS tagging\n",
    "    - NER\n",
    "    - Parsing\n",
    "    - Coreference Resolution\n",
    "    \n",
    "\n",
    "4. **Text Pre-Processing on Tweets Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c070ed94",
   "metadata": {},
   "source": [
    "# Download and Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c00384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade pip\n",
    "!{sys.executable} -m pip install -q --upgrade numpy pandas sklearn\n",
    "!{sys.executable} -m pip install -q --upgrade nltk spacy gensim wordcloud textblob contractions clean-text unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4bb58-646a-46de-9954-b44b4104150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacbab4-2915-435d-9bea-61ff0056e2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc06020-5fb2-4417-a87a-c5a71839040d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760ac46f-ec0f-4d2e-aa26-e80d2e448a1b",
   "metadata": {},
   "source": [
    "# 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4d31d-6e9c-499a-a542-9c5e6fbfd889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60f248-b8d4-498e-8817-eff66b1182c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fdb10-764f-4639-984c-c5d3b22f6a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29fcb284",
   "metadata": {},
   "source": [
    "## a. Removing Digits and Words Containing Digits \n",
    "- Sometimes it happens that words and digits combine are written in the text which creates a problem for machines to understand. Hence, we need to remove the words and digits which are combined like game57 or game5ts7.\n",
    "- For such and many other tasks we normally use Regular Expressions.\n",
    "- Watch  two videos on regular expressions:\n",
    "    - https://www.youtube.com/watch?v=DhQ-kc6FPVk\n",
    "    - https://www.youtube.com/watch?v=3J62z5aGTQc\n",
    "\n",
    "- The **`re.sub(pattern, replacement_string, str)`** method return the string obtained by replacing the occurrences of `pattern` in `str` with the `replacement_string`. If the pattern isn’t found, the string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3827cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  a  string containing  words   having digits'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"This is abc32 a abc32xyz string containing 32abc words  32 having digits\"\n",
    "re.sub('\\w*\\d\\w*', '', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93101cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86620db8-970c-4cf0-b571-79349660b523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is abc32 a abc32xyz string containing 32a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having digits in this sentence 123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  This is abc32 a abc32xyz string containing 32a...      0\n",
       "1                 Having digits in this sentence 123      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = {\n",
    "    'text': [\"This is abc32 a abc32xyz string containing 32abc words\", \"Having digits in this sentence 123\"],\n",
    "    'label': [0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c736d9f-f805-4c70-b943-e93e24e94cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832f00f7-96ab-4b0e-8609-568613197235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove words containing digits\n",
    "def remove_words_with_digits(text):\n",
    "    return re.sub(r'\\b\\w*\\d\\w*\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f863fa65-69b5-403b-af18-7339c4771b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is  a  string containing  words</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having digits in this sentence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text  label\n",
       "0  This is  a  string containing  words      0\n",
       "1       Having digits in this sentence       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(remove_words_with_digits)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59570ff-3d76-4375-8554-3bfedf48a07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d279225",
   "metadata": {},
   "source": [
    "## b. Removing New Line Characters and Extra Spaces\n",
    "- Most of the time text data contain extra spaces or while removing digits more than one space is left between the text.\n",
    "- We can use Python's string and re module to perform this pre-processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f5e972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a string with lots of extra spaces in beteween words .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"      This         is a       string  with   lots of   extra spaces      in beteween    words     .\"\n",
    "re.sub(' +', ' ', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785a2d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      " This is\n",
      "a string\n",
      "with lots of new\n",
      "line characters.\n",
      "Preprocessed String: This is a string with lots of new line characters.\n"
     ]
    }
   ],
   "source": [
    "mystr = \"This is\\na string\\nwith lots of new\\nline characters.\"\n",
    "print(\"Original String:\\n\", mystr)\n",
    "print(\"Preprocessed String:\", re.sub('\\n', ' ', mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52015e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dee5c0a",
   "metadata": {},
   "source": [
    "## c. Removing HTML Tags\n",
    "- Once you get data via scraping websites, your data might contain HTML tags, which are not required as such in the data. So we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "682dcc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:  <html> <head> An empty head. </head><body><p> This is so simple and fun. </p> </body> </html>\n",
      "Preprocessed String:    An empty head.  This is so simple and fun.   \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"<html> <head> An empty head. </head><body><p> This is so simple and fun. </p> </body> </html>\"\n",
    "print(\"Original String: \", mystr)\n",
    "print(\"Preprocessed String: \", re.sub('<.*?>', '', mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca00acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bad8d0-b558-4b88-ba91-a2e9b87b024c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. The filming tec...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "\n",
    "# Function to remove HTML tags from a string using regular expressions\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# Display the DataFrame after removing HTML tags\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95477826-edf8-4da5-a24d-477d6724ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. The filming tec...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "\n",
    "# Function to remove HTML tags from a string using regular expressions\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return clean.sub(r'', text)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# Display the DataFrame after removing HTML tags\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61bd17-ace3-4850-8db4-ae24a6daf8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055dbd84-9ad4-42e0-9f5e-73e350888589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. The filming tec...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "\n",
    "# Function to remove HTML tags from a string\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# Display the DataFrame after removing HTML tags\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285b3ce-fcec-49da-a52a-1c972354a4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "686bf15c-aeaa-4287-85ea-650acc4b0229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from html import unescape\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "\n",
    "# Function to remove HTML tags from a string\n",
    "def remove_html_tags(text):\n",
    "    return unescape(text)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# Display the DataFrame after removing HTML tags\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99706b30-703b-48d5-b61b-075fd7f2d4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705acbce-6fbd-45a7-b7d6-b907aaa8454a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167b6180",
   "metadata": {},
   "source": [
    "## d. Removing URLs\n",
    "- At times the text data you have some URLS, which might not be helpful in suppose sentiment analysis. So better to remove those URLS from your dataset\n",
    "- Once again, we can use Python's re module to remove the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f73fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good youTube lectures by Arif are available at '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"Good youTube lectures by Arif are available at http://www.youtube.com/c/LearnWithArif/playlists\"\n",
    "re.sub('https?://\\S+|www.\\.\\S+', '', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9c503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_4000\\2554164819.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Good YouTube lectures by Arif are available at http://www.youtube.com/c/LearnWithArif/playlists',\n",
       "        'positive']], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mystr = \"Good YouTube lectures by Arif are available at http://www.youtube.com/c/LearnWithArif/playlists\"\n",
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "new_row = {'review': mystr, 'sentiment': 'positive'}\n",
    "df = df.append(new_row, ignore_index=True)\n",
    "df.tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8cd2cde-9259-48ab-b01b-6a143cca21bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Good YouTube lectures by Arif are available at '], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www.\\.\\S+', '', text)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_urls)\n",
    "df['review'].tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cc3ea-4326-4d0d-b8c7-2b6655cf1dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7300c37-85bb-4622-9fd1-0eccea12b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e0e219",
   "metadata": {},
   "source": [
    "## e. Removing Punctuations\n",
    "- Punctuations are symbols that are used to divide written words into sentences and clauses\n",
    "- Once you tokenize your text, these punctuation symbols may become part of a token, and may become a token by itself, which is not required in most of the cases\n",
    "- We can use Python's `string.punctuation` constant and `replace()` method to replace any punctuation in text with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b06a5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067f95f",
   "metadata": {},
   "source": [
    ">- Check for other constants like `string.whitespace`, `string.printable`, `string.ascii_letters`, `string.digits` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f123fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = 'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76aa3beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A text having lot of s and puncutations'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newstr = ''.join([ch for ch in mystr if ch not in string.punctuation])\n",
    "newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e264686-e90c-4fca-8166-dd9a3dfa4ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "247adf59-ddf4-429c-ab1b-043c3feeab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_4000\\2463593850.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..',\n",
       "        'positive']], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "mystr = 'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'\n",
    "\n",
    "df = pd.read_csv(\"datasets/IMDBDataset1.csv\")\n",
    "\n",
    "new_row = {'review': mystr, 'sentiment': 'positive'}\n",
    "\n",
    "df = df.append(new_row, ignore_index=True)\n",
    "df.tail(1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f92f9b-c3b9-433d-8ac1-3a9d603d8a47",
   "metadata": {},
   "source": [
    "### Most efficient in speed method to remove punctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c34bfa-55b2-48c4-b643-212a4ea4a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A text having lot of s and puncutations', 'positive']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove punctuations from a string using string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    exclude = string.punctuation\n",
    "    translator = str.maketrans('', '', exclude)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['review'] = df['review'].apply(remove_punctuations)\n",
    "\n",
    "# Display the DataFrame after removing punctuations\n",
    "df.tail(1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925565e2-02e7-45bc-8644-48eb7a92de39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afc171-c985-43e7-b4aa-4daba4903a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438669d8-d01d-477a-ace0-0ed6051cd3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df149851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_4000\\3991150141.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..',\n",
       "        'positive']], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = 'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'\n",
    "new_row = {'review': mystr, 'sentiment': 'positive'}\n",
    "df = df.append(new_row, ignore_index=True)\n",
    "df.tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7642db87-9d99-4281-a1e8-53d9787fa7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Im going to have to disagree with the previous...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>A text having lot of s and puncutations</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>A text having lot of s and puncutations</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  Im going to have to disagree with the previous...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "50000            A text having lot of s and puncutations  positive\n",
       "50001            A text having lot of s and puncutations  positive"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "df['review'] = df['review'].apply(remove_punctuations)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0b8a7f2-b6d8-4808-b51e-1f6bf9763ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A text having lot of s and puncutations', 'positive']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365272f-9eea-4cff-867b-72936e8e7bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f28d73",
   "metadata": {},
   "source": [
    "# 2. Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b42da8",
   "metadata": {},
   "source": [
    "## a. Case Folding \n",
    "- The text we need to process may come in lower, upper, sentence, camel cases\n",
    "- If the text is in the same case, it is easy for a machine to interpret the words because the lower case and upper case are treated differently by the machine\n",
    "- In applications like Information Retrieval, we reduce all letters to lower case\n",
    "- In applications like sentiment analysis, machine translation and information extraction, keeping the case might be helpful. For example US vs us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb6c75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text for lowercasing.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an Example Text for Lowercasing.\"\n",
    "lowercased_text = text.lower()\n",
    "print(lowercased_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b34d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b6df07-43c1-4216-9dd1-ef4696dc69e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/imdb_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fde8992-1a27-427c-a439-8d54e6d1ab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'petter mattei\\'s \"love in the time of money\" is a visually stunning film to watch. mr. mattei offers us a vivid portrait about human relations. this is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />this being a variation on the arthur schnitzler\\'s play about the same theme, the director transfers the action to the present time new york where all these different characters meet and connect. each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. stylishly, the film has a sophisticated luxurious look. we are taken to see how these people live and the world they live in their own habitat.<br /><br />the only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. a big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />the acting is good under mr. mattei\\'s direction. steve buscemi, rosario dawson, carol kane, michael imperioli, adrian grenier, and the rest of the talented cast, make these characters come alive.<br /><br />we wish mr. mattei good luck and await anxiously for his next work.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][4].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9994a2-ab44-46ca-882e-e7c43cb5d45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      one of the other reviewers has mentioned that ...\n",
       "1      a wonderful little production. <br /><br />the...\n",
       "2      i thought this was a wonderful way to spend ti...\n",
       "3      basically there's a family where a little boy ...\n",
       "4      petter mattei's \"love in the time of money\" is...\n",
       "                             ...                        \n",
       "994    on watching this film, i was amazed at how med...\n",
       "995    nothing is sacred. just ask ernie fosselius. t...\n",
       "996    i hated it. i hate self-aware pretentious inan...\n",
       "997    i usually try to be professional and construct...\n",
       "998    if you like me is going to see this in a film ...\n",
       "Name: review, Length: 999, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb08f800-3207-4f1b-84ee-d98986e1ebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>on watching this film, i was amazed at how med...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>nothing is sacred. just ask ernie fosselius. t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>i hated it. i hate self-aware pretentious inan...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>i usually try to be professional and construct...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>if you like me is going to see this in a film ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one of the other reviewers has mentioned that ...  positive\n",
       "1    a wonderful little production. <br /><br />the...  positive\n",
       "2    i thought this was a wonderful way to spend ti...  positive\n",
       "3    basically there's a family where a little boy ...  negative\n",
       "4    petter mattei's \"love in the time of money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "994  on watching this film, i was amazed at how med...  positive\n",
       "995  nothing is sacred. just ask ernie fosselius. t...  positive\n",
       "996  i hated it. i hate self-aware pretentious inan...  negative\n",
       "997  i usually try to be professional and construct...  negative\n",
       "998  if you like me is going to see this in a film ...  negative\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6faf715-5eb4-4d75-acb9-028ec32f4aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aeb050-950a-4270-93f4-8b9b197d81fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fd645b",
   "metadata": {},
   "source": [
    "## b. Expand Contractions\n",
    "- Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
    "- Examples:\n",
    "    - you're ---> you are\n",
    "    - ain't ---> am not / are not / is not / has not / have not\n",
    "    - you'll ---> you shall / you will\n",
    "    - wouldn't 've ---> would not haveyou are\n",
    "- In order to expand contractions, you can install and use the `contractions` module or can create your own dictionary to expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c90f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bec3e307-0b3d-45d4-83f4-836b757d5f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q  contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b5ff1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are\n",
      "are not\n",
      "you will\n",
      "would not have\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "print(contractions.fix(\"you're\"))      # you are\n",
    "print(contractions.fix(\"ain't\"))       # am not / are not / is not / has not / have not\n",
    "print(contractions.fix(\"you'll\"))      #you shall / you will\n",
    "print(contractions.fix(\"wouldn't've\")) #\"wouldn't've\": \"would not have\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8cc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6b0d807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \\nIt's awesome to meet new friends. We've been waiting for this day for so long.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = '''I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \n",
    "It's awesome to meet new friends. We've been waiting for this day for so long.'''\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128dec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be there within 5 min. Should not you be there too? I would love to see you there my dear. \n",
      "It is awesome to meet new friends. We have been waiting for this day for so long.\n"
     ]
    }
   ],
   "source": [
    "# use loop\n",
    "mylist = []   \n",
    "for word in mystr.split(sep=' '):\n",
    "    mylist.append(contractions.fix(word))\n",
    "\n",
    "newstring = ' '.join(mylist)\n",
    "print(newstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61d4bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use list comprehension and join the words of list on space\n",
    "expanded_string = ' '.join([contractions.fix(word) for word in mystr.split()])\n",
    "expanded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a0fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52da3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b864e428",
   "metadata": {},
   "source": [
    "## c. Chat Word Treatment\n",
    "- Some commonly used abbreviated chat words that are used on social media these days are:\n",
    "    - GN for good night\n",
    "    - fyi for for your information\n",
    "    - asap for as soon as possible\n",
    "    - yolo for you only live once\n",
    "    - rofl for rolling on floor laughing\n",
    "    - nvm for never mind\n",
    "    - ofc for ofcourse\n",
    "\n",
    "- To pre-process any text containing such abbreviations we can search for an online dictionary, or can create a dictionary of our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d8f4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_chatwords = { \n",
    "    'ack': 'acknowledge',\n",
    "    'omg': 'oh my God',\n",
    "    'aisi': 'as i see it',\n",
    "    'bi5': 'back in 5 minutes',\n",
    "    'lmk': 'let me know',\n",
    "    'gn' : 'good night',\n",
    "    'fyi': 'for your information',\n",
    "    'asap': 'as soon as possible',\n",
    "    'yolo': 'you only live once',\n",
    "    'rofl': 'rolling on floor laughing',\n",
    "    'nvm': 'never ming',\n",
    "    'ofc': 'ofcourse',\n",
    "    'blv' : 'boulevard',\n",
    "    'cir' : 'circle',\n",
    "    'hwy' : 'highway',\n",
    "    'ln' : 'lane',\n",
    "    'pt' : 'point',\n",
    "    'rd' : 'road',\n",
    "    'sq' : 'square',\n",
    "    'st' : 'street'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "560fb80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'omg this is aisi I ack your work and will be bi5'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"omg this is aisi I ack your work and will be bi5\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74246a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict.items() method returns all the key-value pairs of a dict as a two object tuple\n",
    "# dict.keys() method returns all the keys  of a dict object\n",
    "# dict.values() method returns all the values  of a dict object\n",
    "def replace_chat_words(text, chat_dict):\n",
    "    mylist = []   \n",
    "    for word in mystr.split(sep=' '):\n",
    "        if word in dict_chatwords.keys():\n",
    "            mylist.append(dict_chatwords[word])\n",
    "        else:\n",
    "            mylist.append(word)\n",
    "    newstring = ' '.join(mylist)\n",
    "    return newstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fee8b5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh my God this is as i see it I acknowledge your work and will be back in 5 minutes'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_chat_words(mystr,dict_chatwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40839642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omg! lmk asap. gn yolo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fyi, ack received. rd ahead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bi5, will be back soon. nvm the delay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hwy to blv or cir?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rofl, that's hilarious! ln st rd.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text\n",
       "0                 omg! lmk asap. gn yolo!\n",
       "1            fyi, ack received. rd ahead.\n",
       "2  bi5, will be back soon. nvm the delay.\n",
       "3                      hwy to blv or cir?\n",
       "4       rofl, that's hilarious! ln st rd."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"omg! lmk asap. gn yolo!\",\n",
    "    \"fyi, ack received. rd ahead.\",\n",
    "    \"bi5, will be back soon. nvm the delay.\",\n",
    "    \"hwy to blv or cir?\",\n",
    "    \"rofl, that's hilarious! ln st rd.\",\n",
    "    \"ofc, it's a sq day.\",\n",
    "    \"nvm, cir and rd are the same.\",\n",
    "    \"lmk if you can join. st address.\",\n",
    "    \"gn! see you tomorrow. rd trip!\",\n",
    "    \"aisi, yolo! gn all!\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14766310-4d74-46ee-8ca8-31e75cff82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chat_words(text, chat_dict):\n",
    "    mylist = []\n",
    "    for word in text.split(sep=' '):\n",
    "        if word in chat_dict.keys():\n",
    "            mylist.append(chat_dict[word])\n",
    "        else:\n",
    "            mylist.append(word)\n",
    "    new_string = ' '.join(mylist)\n",
    "    return new_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab361d1c-2f6b-4fa4-b564-cde86bebe532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omg! let me know asap. good night yolo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fyi, acknowledge received. road ahead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bi5, will be back soon. never ming the delay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>highway to boulevard or cir?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rofl, that's hilarious! lane street rd.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text\n",
       "0        omg! let me know asap. good night yolo!\n",
       "1         fyi, acknowledge received. road ahead.\n",
       "2  bi5, will be back soon. never ming the delay.\n",
       "3                   highway to boulevard or cir?\n",
       "4        rofl, that's hilarious! lane street rd."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(replace_chat_words, chat_dict=dict_chatwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45a43ede-b93c-47e6-8fd5-4f56cc912f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text\n",
      "0        omg! let me know asap. good night yolo!\n",
      "1         fyi, acknowledge received. road ahead.\n",
      "2  bi5, will be back soon. never mind the delay.\n",
      "3                   highway to boulevard or cir?\n",
      "4        rofl, that's hilarious! lane street rd.\n",
      "5                        ofc, it's a square day.\n",
      "6             nvm, circle and road are the same.\n",
      "7   let me know if you can join. street address.\n",
      "8               gn! see you tomorrow. road trip!\n",
      "9                    aisi, yolo! good night all!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary of chat words\n",
    "dict_chatwords = {\n",
    "    'ack': 'acknowledge',\n",
    "    'omg': 'oh my God',\n",
    "    'aisi': 'as i see it',\n",
    "    'bi5': 'back in 5 minutes',\n",
    "    'lmk': 'let me know',\n",
    "    'gn': 'good night',\n",
    "    'fyi': 'for your information',\n",
    "    'asap': 'as soon as possible',\n",
    "    'yolo': 'you only live once',\n",
    "    'rofl': 'rolling on floor laughing',\n",
    "    'nvm': 'never mind',\n",
    "    'ofc': 'of course',\n",
    "    'blv': 'boulevard',\n",
    "    'cir': 'circle',\n",
    "    'hwy': 'highway',\n",
    "    'ln': 'lane',\n",
    "    'pt': 'point',\n",
    "    'rd': 'road',\n",
    "    'sq': 'square',\n",
    "    'st': 'street'\n",
    "}\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"omg! lmk asap. gn yolo!\",\n",
    "    \"fyi, ack received. rd ahead.\",\n",
    "    \"bi5, will be back soon. nvm the delay.\",\n",
    "    \"hwy to blv or cir?\",\n",
    "    \"rofl, that's hilarious! ln st rd.\",\n",
    "    \"ofc, it's a sq day.\",\n",
    "    \"nvm, cir and rd are the same.\",\n",
    "    \"lmk if you can join. st address.\",\n",
    "    \"gn! see you tomorrow. rd trip!\",\n",
    "    \"aisi, yolo! gn all!\"\n",
    "]\n",
    "\n",
    "def replace_chat_words(text, chat_dict):\n",
    "    mylist = []   \n",
    "    for word in text.split(sep=' '):\n",
    "        if word in chat_dict.keys():\n",
    "            mylist.append(chat_dict[word])\n",
    "        else:\n",
    "            mylist.append(word)\n",
    "    newstring = ' '.join(mylist)\n",
    "    return newstring\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts})\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: replace_chat_words(x, dict_chatwords))\n",
    "\n",
    "# Display the DataFrame after replacing chat words\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630067c6-b92a-415f-9e49-73982eadece7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a80d754-f5f1-48f5-a15c-9d155c047dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  oh my God! let me know as soon as possible. go...\n",
      "1  for your information, acknowledge received. ro...\n",
      "2  back in 5 minutes, will be back soon. never mi...\n",
      "3                    highway to boulevard or circle?\n",
      "4  rolling on floor laughing, that's hilarious! l...\n",
      "5                      of course, it's a square day.\n",
      "6          never mind, circle and road are the same.\n",
      "7       let me know if you can join. street address.\n",
      "8           good night! see you tomorrow. road trip!\n",
      "9   as i see it, you only live once! good night all!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Dictionary of chat words\n",
    "dict_chatwords = {\n",
    "    'ack': 'acknowledge',\n",
    "    'omg': 'oh my God',\n",
    "    'aisi': 'as i see it',\n",
    "    'bi5': 'back in 5 minutes',\n",
    "    'lmk': 'let me know',\n",
    "    'gn': 'good night',\n",
    "    'fyi': 'for your information',\n",
    "    'asap': 'as soon as possible',\n",
    "    'yolo': 'you only live once',\n",
    "    'rofl': 'rolling on floor laughing',\n",
    "    'nvm': 'never mind',\n",
    "    'ofc': 'of course',\n",
    "    'blv': 'boulevard',\n",
    "    'cir': 'circle',\n",
    "    'hwy': 'highway',\n",
    "    'ln': 'lane',\n",
    "    'pt': 'point',\n",
    "    'rd': 'road',\n",
    "    'sq': 'square',\n",
    "    'st': 'street'\n",
    "}\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"omg! lmk asap. gn yolo!\",\n",
    "    \"fyi, ack received. rd ahead.\",\n",
    "    \"bi5, will be back soon. nvm the delay.\",\n",
    "    \"hwy to blv or cir?\",\n",
    "    \"rofl, that's hilarious! ln st rd.\",\n",
    "    \"ofc, it's a sq day.\",\n",
    "    \"nvm, cir and rd are the same.\",\n",
    "    \"lmk if you can join. st address.\",\n",
    "    \"gn! see you tomorrow. rd trip!\",\n",
    "    \"aisi, yolo! gn all!\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts})\n",
    "\n",
    "# Function to replace chat words\n",
    "def replace_chat_words(text, chat_dict):\n",
    "    for word, replacement in chat_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(word) + r'\\b', replacement, text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: replace_chat_words(x, dict_chatwords))\n",
    "\n",
    "# Display the DataFrame after replacing chat words\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fd809-5166-4608-8d53-1b25cb19e1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f62553",
   "metadata": {},
   "source": [
    "## d. Handle Emojis\n",
    "- We come across lots and lots of emojis while scraping comments/posts from social media websites like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, which needs to be removed from text.\n",
    "- Machine Learrning algorithm cannot understand emojis, so we have two options:\n",
    "    - Simply remove the emojis from the text data, and this can be done using `clean-text` library\n",
    "    - Replace the emoji with its meaning happy, sad, angry,....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bf235",
   "metadata": {},
   "source": [
    "### (i) Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4294457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These emojis needs to be removed, there is a huge list...😃😬😂😅😇😉😊😜😎🤗🙄🤔😡😤😭🤠🤡🤫💩😈👻🙌👍✌️👌🙏'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"These emojis needs to be removed, there is a huge list...😃😬😂😅😇😉😊😜😎🤗🙄🤔😡😤😭🤠🤡🤫💩😈👻🙌👍✌️👌🙏\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8be07245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These emojis needs to be removed, there is a huge list...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # code range for emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # code range for symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # code range for transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # code range for flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # code range for Dingbats\n",
    "        u\"\\U00002500-\\U00002BEF\"  # code range for chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "print(emoji_pattern.sub(r'', mystr)) # no emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbbf5c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello! 😀 How are you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming! 💻🚀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a test message with 😊 emojis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>🌍 Traveling around the world! ✈️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python programming is fun! 🐍</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text\n",
       "0            Hello! 😀 How are you today?\n",
       "1                 I love programming! 💻🚀\n",
       "2  This is a test message with 😊 emojis.\n",
       "3       🌍 Traveling around the world! ✈️\n",
       "4           Python programming is fun! 🐍"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample text containing emojis\n",
    "texts_with_emojis = [\n",
    "    \"Hello! 😀 How are you today?\",\n",
    "    \"I love programming! 💻🚀\",\n",
    "    \"This is a test message with 😊 emojis.\",\n",
    "    \"🌍 Traveling around the world! ✈️\",\n",
    "    \"Python programming is fun! 🐍\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts_with_emojis})\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d83c72e-7279-4c8b-9a74-aa9976a2c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern for emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u2640-\\u2642\" \n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\ufe0f\" \n",
    "    u\"\\u3030\"\n",
    "    \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55fa2113-bcc6-49c8-adf8-ba03e642d924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello!  How are you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a test message with  emojis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Traveling around the world!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python programming is fun!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0            Hello!  How are you today?\n",
       "1                  I love programming! \n",
       "2  This is a test message with  emojis.\n",
       "3          Traveling around the world! \n",
       "4           Python programming is fun! "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7e881",
   "metadata": {},
   "source": [
    "### (ii) Replace Emojis with their Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "059582dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a02e204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  :thumbs_up:'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "mystr = \"This is  👍\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c6992f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am :thinking_face:'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"I am 🤔\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f4551af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  positive'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"This is  👍\"\n",
    "emoji.replace_emoji(mystr, replace='positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb98ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da1235-0b1a-4e45-a4d5-764cb34607bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eff2330-faf5-4700-a65a-7d327124fb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello! 😀 How are you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming! 💻🚀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a test message with 😊 emojis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>🌍 Traveling around the world! ✈️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python programming is fun! 🐍</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text\n",
       "0            Hello! 😀 How are you today?\n",
       "1                 I love programming! 💻🚀\n",
       "2  This is a test message with 😊 emojis.\n",
       "3       🌍 Traveling around the world! ✈️\n",
       "4           Python programming is fun! 🐍"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample text containing emojis\n",
    "texts_with_emojis = [\n",
    "    \"Hello! 😀 How are you today?\",\n",
    "    \"I love programming! 💻🚀\",\n",
    "    \"This is a test message with 😊 emojis.\",\n",
    "    \"🌍 Traveling around the world! ✈️\",\n",
    "    \"Python programming is fun! 🐍\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts_with_emojis})\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47113a-0684-4ed5-bd7e-c8f9b95842ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72dc4cb2-d609-4e6f-a803-7599062b361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello! :grinning_face: How are you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming! :laptop::rocket:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a test message with :smiling_face_with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:globe_showing_Europe-Africa: Traveling around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python programming is fun! :snake:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0          Hello! :grinning_face: How are you today?\n",
       "1               I love programming! :laptop::rocket:\n",
       "2  This is a test message with :smiling_face_with...\n",
       "3  :globe_showing_Europe-Africa: Traveling around...\n",
       "4                 Python programming is fun! :snake:"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(convert_emojis_to_text)\n",
    "\n",
    "# Display the DataFrame after removing emojis\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732269b-93c3-4f5d-bbc7-917db81584fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89b345-6128-4f5b-87d1-8268f3e9ce5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181c15ca",
   "metadata": {},
   "source": [
    "## e. Spelling Correction\n",
    "- Most of the times the text data you have contain spelling errors, which if not corrected the same word may be represented in two or may be more different ways.\n",
    "- Almost all word editors, today underline incorrectly typed words and provide you possible correct options\n",
    "- So spelling correction is a two step task:\n",
    "    - Detection of spelling errors\n",
    "    - Correction of spelling errors\n",
    "        - Autocorrect as you type space\n",
    "        - Suggest a single correct word\n",
    "        - Suggest a list of words (from which you can choose one)\n",
    "- Types of spelling errors:\n",
    "    - **Non-word Errors:** are non-dictionary words or words that do not exist in the language dictionary. For example instead of typing `reading` the user typed `reeding`. These are easy to detect as they do not exist in the language dictionary and can be corrected using algorithms like shortest weighted edit distance and highest noisy channel probability.\n",
    "    - **Real-word Errors:** are dictionary words and are hard to detect. These can be of two types:\n",
    "        - Typographical errors: For example instead of typing `great` the user typed `greet`\n",
    "        - Cognitive errors (homophones: For example instead of typing `two` the user typed `too`\n",
    "\n",
    "\n",
    "<h2 align=\"left\" style=\"font-family:'Arial'\">\"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db931961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9985af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "458947c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1362f338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "textblob.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89720d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "mystr = \"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"\n",
    "blob = TextBlob(mystr)\n",
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69042422-2afd-493e-9832-93e254452d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af745e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cmpkey', '_compare', '_create_sentence_objects', '_strkey', 'analyzer', 'classifier', 'classify', 'correct', 'detect_language', 'ends_with', 'endswith', 'find', 'format', 'index', 'join', 'json', 'lower', 'ngrams', 'noun_phrases', 'np_counts', 'np_extractor', 'parse', 'parser', 'polarity', 'pos_tagger', 'pos_tags', 'raw', 'raw_sentences', 'replace', 'rfind', 'rindex', 'sentences', 'sentiment', 'sentiment_assessments', 'serialized', 'split', 'starts_with', 'startswith', 'string', 'strip', 'stripped', 'subjectivity', 'tags', 'title', 'to_json', 'tokenize', 'tokenizer', 'tokens', 'translate', 'translator', 'upper', 'word_counts', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(blob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4813310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am reading this great book on data science subject, which is a greet curse'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d58342",
   "metadata": {},
   "source": [
    ">-  The non-word errors like `reeding`, `this`, `gret`, `boook`, `deta`, `sciance` and `suject` have been corrected by `blob.correct()` method\n",
    ">- However, the real word errors like `greet` and `curse` are not corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed0157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d0b85cf",
   "metadata": {},
   "source": [
    "**Let us try to understand how `textblob.correct()` method do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28586b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['I', 'am', 'reeding', 'thiss', 'gret', 'boook', 'on', 'deta', 'sciance', 'suject', 'which', 'is', 'a', 'greet', 'curse'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word attribute of textblob object returns list of words in the text\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f7dd8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reading', 0.7651006711409396),\n",
       " ('feeding', 0.10067114093959731),\n",
       " ('heeding', 0.053691275167785234),\n",
       " ('rending', 0.026845637583892617),\n",
       " ('breeding', 0.026845637583892617),\n",
       " ('receding', 0.013422818791946308),\n",
       " ('reeling', 0.006711409395973154),\n",
       " ('needing', 0.006711409395973154)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'reeding'\n",
    "blob.words[2].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cea1786c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 0.946969696969697), ('brook', 0.05303030303030303)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'boook'\n",
    "blob.words[5].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "782b98fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greet', 1.0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'greet'\n",
    "blob.words[13].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf78a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86328b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6baf968-ca09-4d92-9dcf-50e04cc03b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4aac22e1-df3f-4422-8d7e-e8becd6b0a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thiss is a testt sentence with wrong spellings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I lovve programing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick browwn fox jumps ovver the lazy dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Helo! Howw are you todai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pythonn programing is funn!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text\n",
       "0  Thiss is a testt sentence with wrong spellings.\n",
       "1                              I lovve programing!\n",
       "2   The quick browwn fox jumps ovver the lazy dog.\n",
       "3                        Helo! Howw are you todai?\n",
       "4                      Pythonn programing is funn!"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "texts = [\n",
    "    \"Thiss is a testt sentence with wrong spellings.\",\n",
    "    \"I lovve programing!\",\n",
    "    \"The quick browwn fox jumps ovver the lazy dog.\",\n",
    "    \"Helo! Howw are you todai?\",\n",
    "    \"Pythonn programing is funn!\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the 'text' column\n",
    "df = pd.DataFrame({'text': texts})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7ab0ff3-1db2-4ed7-a3e0-7df5c41ee935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thiss is a testt sentence with wrong spellings.</td>\n",
       "      <td>Hiss is a test sentence with wrong swellings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I lovve programing!</td>\n",
       "      <td>I love programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick browwn fox jumps ovver the lazy dog.</td>\n",
       "      <td>The quick brown fox jumps over the lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Helo! Howw are you todai?</td>\n",
       "      <td>Felo Now are you today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pythonn programing is funn!</td>\n",
       "      <td>Pythonn programming is funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  \\\n",
       "0  Thiss is a testt sentence with wrong spellings.   \n",
       "1                              I lovve programing!   \n",
       "2   The quick browwn fox jumps ovver the lazy dog.   \n",
       "3                        Helo! Howw are you todai?   \n",
       "4                      Pythonn programing is funn!   \n",
       "\n",
       "                                 corrected_text  \n",
       "0  Hiss is a test sentence with wrong swellings  \n",
       "1                            I love programming  \n",
       "2   The quick brown fox jumps over the lazy dog  \n",
       "3                        Felo Now are you today  \n",
       "4                  Pythonn programming is funny  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to correct spellings using TextBlob\n",
    "def correct_spellings(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = ' '.join([str(word.correct()) for word in blob.words])\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "df['corrected_text'] = df['text'].apply(correct_spellings)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d6448",
   "metadata": {},
   "source": [
    "## f. Tokenize Text\n",
    "\n",
    "<img align=right src=\"images/tokenization.png\" width=\"500\">\n",
    "\n",
    "- **What is Tokenization:** Tokenization is a process of splitting text into meaningful segments called tokens. It can be character level, subword level, word level (unigram), two word level (bigram), three word level (trigram), and sentence level.\n",
    "- **Why to do Tokenization:** For classification of a product review as positive or negative, we may need to count the number of positive words and compare them with the count of negative words in the text of that review. For this we first need to tokenize the text of the product review. Tokens are the basic uilding locks of a document oject. Everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another.\n",
    "- **How to do Tokenization:** In a sentence you may come across following four items:\n",
    "    -  **Prefix**:\tCharacter(s) at the beginning &#9656; `( “ $ Rs Dr`\n",
    "    -  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ”`\n",
    "    -  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "    -  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. From `L.A.!` the exclamation mark (!) is separated, while `L.A.` is not split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14543dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e41f20c8",
   "metadata": {},
   "source": [
    "### (i) Tokenization with `string.split()` Method\n",
    "- The easiest way to tokenize is to use the `mystr.split()` method, which returns a list of strings.\n",
    "- The `mystr.split()` method splits a string into a list of strings at every occurrence of space character by default and discard empty strings from the result.\n",
    "- You may pass a parameter `sep='i'` to split method to split at that specific character instead.\n",
    "- It's limitation is that it do not  consider punctuation symbols as a separate token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6edf5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Learning', 'is', 'fun', 'with', 'Arif']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"Learning is fun with Arif\" \n",
    "print(mystr.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "262a4014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'is', 'great!']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"This example is great!\" \n",
    "print(mystr.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7855c",
   "metadata": {},
   "source": [
    "> <font color=green> Observe the output, the exclamation symbol has become part of the token great (which is wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0bb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f65b7fe",
   "metadata": {},
   "source": [
    "### (ii) Tokenization with `re.split()` Method\n",
    "- The `re.split()` method splits the source string by the occurrences of the pattern, returning a list containing the resulting substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78f6ce39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'is', 'great', '']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr=\"This example is great!\" \n",
    "pattern = re.compile(r'\\W+')\n",
    "pattern.split(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b8f71",
   "metadata": {},
   "source": [
    ">- <font color=green> The exclamation symbol is not part of the token great, but what if I need that symbol as a separate token?\n",
    ">- <font color=green> Moreover, you need to write different regular expression for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf7d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf29a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a51dd",
   "metadata": {},
   "source": [
    "### (iii) Tokenization using NLTK\n",
    "- NLTK stands for Natural Language Toolkit (https://www.nltk.org/). This is a suite of libraries and programs for statistical natural language processing for English language\n",
    "- NLTK was released in 2001, and is available for Windows, Mac OS X, and Linux.. \n",
    "- NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "- NLTK fully supports the English language, but others like Spanish or French are not supported as extensively.\n",
    "- It is a string processing libbrary, i.e., you give a string as input and get a string as output\n",
    "- There are. different tokenizer available in nltk:\n",
    "    - `nltk.tokenize.sent_tokenize(str)` for sentence tokenization\n",
    "    - `nltk.tokenize.word_tokenize(str)` for word tokenization\n",
    "    - `nltk.tokenize.treebank.TreebankWordTokenizer(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb11801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea7d7775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd2a097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'is', 'great', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "mystr=\"This example is great!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8481ee7",
   "metadata": {},
   "source": [
    "> <font color=green> Observe the output, this time the exclamation symbol is kept as a separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16a08c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'should', 'do', 'your', 'Ph.D', 'in', 'A.I', '!']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"You should do your Ph.D in A.I!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c34b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'should', \"'ve\", 'sent', 'me', 'an', 'email', 'at', 'arif', '@', 'pucit.edu.pk', 'or', 'vist', 'http', ':', '//www/arifbutt.me']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"You should've sent me an email at arif@pucit.edu.pk or vist http://www/arifbutt.me\"\n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d01562a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', \"'s\", 'an', 'example', 'worth', '$', '100', '.', 'I', 'am', '384400km', 'away', 'from', 'earth', \"'s\", 'moon', '!']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"Here's an example worth $100. I am 384400km away from earth's moon!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8673997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ddbac7d",
   "metadata": {},
   "source": [
    "### (iv) Tokenization with spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token.\n",
    "\n",
    "- **Download spacy model for English language**\n",
    "    - Spacy comes with pretrained models and pipelines for different languages.\n",
    "    - You can download any of the following models for English language, but better to download the small as this will require a reasonable amount of space on your disk, and may take a bit of time to download:\n",
    "        - en_core_web_sm\n",
    "        - en_core_web_md\n",
    "        - en_core_web_lg\n",
    "        - en_core_web_trf\n",
    "    - The model name consist of four parts:\n",
    "        - Language (en): The language abreviation can be `en` for English, `fr` for French, `zh` for Chinese\n",
    "        - Type (core/dep): It can be core for general-purpose pipeline with tagging, parsing, lemmatization and NER recognition. It can be dep for only tagging, parsing and lemmatization\n",
    "        - Genre (web/news): It measn the type of text the pipeline is trained on, e.g., web or news. \n",
    "        - Size: Package size indicator. `sm` for small, `md` for medium, `lg` for large and `trf for transformer\n",
    "        - Package version (a.b.c): Here a is the major version for spaCy, b is the minor version for spaCy, while c is the model verion dependent to the data on which the model is trained, it parameters, number of iterations and different vectors.\n",
    "        \n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed9f8834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "730ee137-efa1-44b3-810f-c474085871fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5317e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed673218",
   "metadata": {},
   "source": [
    "**Download spacy model for English language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4cfeffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e336a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Program' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a39956",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1eef602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' , A , 7 , km , Uber , cab , ride , from , Gulberg , to , Joher , Town , will , cost , you , $ , 20 , "
     ]
    }
   ],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "mystr=\"'A 7km Uber cab ride from Gulberg to Joher Town will cost you $20\" \n",
    "doc = nlp(mystr)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b29abc",
   "metadata": {},
   "source": [
    "> <font color=green> Note that spacy has successfully tokenized the distance symbol, which nltk failed to separate.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448839ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a09da",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0baf9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You , should , 've , sent , me , an , email , at , arif@pucit.edu.pk , or , vist , http://www , / , arifbutt.me , "
     ]
    }
   ],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "mystr=\"You should've sent me an email at arif@pucit.edu.pk or vist http://www/arifbutt.me\"\n",
    "doc = nlp(mystr)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46130c",
   "metadata": {},
   "source": [
    ">- <font color=green> Note that spacy has kept the email as a single token, while nltk separated it.</font>\n",
    ">- <font color=green> However, spacy also failed to properly tokenize the URL :(</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ccc05-31fd-467a-8883-ed51b26dfeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696ffe-6331-4efd-9964-6d82cc2fe037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ac8635b1-edde-4b64-b403-43377e12a428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural language processing (NLP) is a fascina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial intelligence (AI) and machine learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantum computing holds the potential to revol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The intersection of data science and domain ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep learning models, such as neural networks,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Natural language processing (NLP) is a fascina...\n",
       "1  Artificial intelligence (AI) and machine learn...\n",
       "2  Quantum computing holds the potential to revol...\n",
       "3  The intersection of data science and domain ex...\n",
       "4  Deep learning models, such as neural networks,..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a dataset with advanced-level text\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Natural language processing (NLP) is a fascinating field of study.\",\n",
    "        \"Artificial intelligence (AI) and machine learning (ML) are transforming various industries.\",\n",
    "        \"Quantum computing holds the potential to revolutionize computational capabilities.\",\n",
    "        \"The intersection of data science and domain expertise leads to actionable insights.\",\n",
    "        \"Deep learning models, such as neural networks, excel at complex tasks.\",\n",
    "        \"Blockchain technology ensures transparency and security in financial transactions.\",\n",
    "        \"Augmented reality (AR) enhances the user experience by overlaying digital information onto the real world.\",\n",
    "        \"Biotechnology advancements are paving the way for personalized medicine.\",\n",
    "        \"Renewable energy sources play a crucial role in combating climate change.\",\n",
    "        \"Space exploration and colonization pose exciting challenges for the future.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5d7f990-287c-4b96-8a98-654702fcf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a fascinating field of study.\n",
      "Artificial intelligence (AI) and machine learning (ML) are transforming various industries.\n",
      "Quantum computing holds the potential to revolutionize computational capabilities.\n",
      "The intersection of data science and domain expertise leads to actionable insights.\n",
      "Deep learning models, such as neural networks, excel at complex tasks.\n",
      "Blockchain technology ensures transparency and security in financial transactions.\n",
      "Augmented reality (AR) enhances the user experience by overlaying digital information onto the real world.\n",
      "Biotechnology advancements are paving the way for personalized medicine.\n",
      "Renewable energy sources play a crucial role in combating climate change.\n",
      "Space exploration and colonization pose exciting challenges for the future.\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize text using spaCy\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    print(doc)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['tokens'] = df['text'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0827c447-c4a9-4b1b-a021-a1f9dd9eb78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural language processing (NLP) is a fascina...</td>\n",
       "      <td>[Natural, language, processing, (, NLP, ), is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial intelligence (AI) and machine learn...</td>\n",
       "      <td>[Artificial, intelligence, (, AI, ), and, mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantum computing holds the potential to revol...</td>\n",
       "      <td>[Quantum, computing, holds, the, potential, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The intersection of data science and domain ex...</td>\n",
       "      <td>[The, intersection, of, data, science, and, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep learning models, such as neural networks,...</td>\n",
       "      <td>[Deep, learning, models, ,, such, as, neural, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Natural language processing (NLP) is a fascina...   \n",
       "1  Artificial intelligence (AI) and machine learn...   \n",
       "2  Quantum computing holds the potential to revol...   \n",
       "3  The intersection of data science and domain ex...   \n",
       "4  Deep learning models, such as neural networks,...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Natural, language, processing, (, NLP, ), is,...  \n",
       "1  [Artificial, intelligence, (, AI, ), and, mach...  \n",
       "2  [Quantum, computing, holds, the, potential, to...  \n",
       "3  [The, intersection, of, data, science, and, do...  \n",
       "4  [Deep, learning, models, ,, such, as, neural, ...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63029a8-ab39-4740-99db-33401780fbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e7123-a2a5-4803-93b8-9b0b20685381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdcff9-9b4c-4905-9148-4eb2c4d27dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cb5cc0",
   "metadata": {},
   "source": [
    "**Additional Token Attributes:** Once the string is passed to `nlp()` method of spacy, the tokens of the resulting `doc` object has many other associated attributes other than just tokens:\n",
    "\n",
    "|Tag|Description\n",
    "|:------|:------:\n",
    "|`.text`|The original word text\n",
    "|`.lemma_`|The base form of the word\n",
    "|`.pos_`|The simple part-of-speech tag\n",
    "|`.tag_`|The detailed part-of-speech tag\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits\n",
    "|`.is_alpha`, `is_ascii`, `is_digit`|Token text consists of alphanumeric characters, ASCII characters, digits\n",
    "|`.is_lower`, `is_upper`, `is_title`|Token text is in lowercase, uppercase, titlecase\n",
    "|`.is_punct`, `is_space`, `is_stop`|Token is punctuation, whitespace, stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bf4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0d692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2ca27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c60224",
   "metadata": {},
   "source": [
    "## g. Creating N-grams\n",
    "- **What are n-grams?** \n",
    "    - A sequence of n words, can be bigram, trigram,....\n",
    "- **Why to use n-grams?** \n",
    "    - Capture contextual information (`good food` carries more meaning than just `good` and `food` when observed independently)\n",
    "    - Applications of N-grams:\n",
    "        - Sentence Completion\n",
    "        - Auto Spell Check and correction\n",
    "        - Auto Grammer Check and correction\n",
    "    - Is there a perfect value of n?\n",
    "        - Different types of n-grams are suitable for different types of applications. You should try different n-grams on your data in order to confidently conclude which one works the best among all for your text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36da1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b05d832",
   "metadata": {},
   "source": [
    "- **How to create n-grams?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f32e3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object bigrams at 0x0000029B5BF27140>\n",
      "('Allama', 'Iqbal')\n",
      "('Iqbal', 'was')\n",
      "('was', 'a')\n",
      "('a', 'visionary')\n",
      "('visionary', 'philosopher')\n",
      "('philosopher', 'and')\n",
      "('and', 'politician')\n",
      "('politician', '.')\n",
      "('.', 'Thank')\n",
      "('Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mystr = \"Allama Iqbal was a visionary philosopher and politician. Thank you\"\n",
    "tokens = nltk.tokenize.word_tokenize(mystr)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "print(bgs)\n",
    "for grams in bgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b31af",
   "metadata": {},
   "source": [
    ">- The formula to calculate the count of n-grams in a document is: **`X - N + 1`**, where `X` is the number of words in a given document and `N` is the number of words in n-gram\n",
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 2 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 10\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df993560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was')\n",
      "('Iqbal', 'was', 'a')\n",
      "('was', 'a', 'visionary')\n",
      "('a', 'visionary', 'philosopher')\n",
      "('visionary', 'philosopher', 'and')\n",
      "('philosopher', 'and', 'politician')\n",
      "('and', 'politician', '.')\n",
      "('politician', '.', 'Thank')\n",
      "('.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "tgs = nltk.trigrams(tokens)\n",
    "for grams in tgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de30a5",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 3 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 9\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30e32325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was', 'a')\n",
      "('Iqbal', 'was', 'a', 'visionary')\n",
      "('was', 'a', 'visionary', 'philosopher')\n",
      "('a', 'visionary', 'philosopher', 'and')\n",
      "('visionary', 'philosopher', 'and', 'politician')\n",
      "('philosopher', 'and', 'politician', '.')\n",
      "('and', 'politician', '.', 'Thank')\n",
      "('politician', '.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "ngrams = nltk.ngrams(tokens, 4)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef11306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1591bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Allama Iqbal was a visionary philosopher and p...\n",
       "1  Thank you for your contribution to our community.\n",
       "2        Python is a versatile programming language.\n",
       "3  Data science is an interdisciplinary field tha..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "\n",
    "# Create a DataFrame with a column 'Text'\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"Allama Iqbal was a visionary philosopher and politician.\",\n",
    "        \"Thank you for your contribution to our community.\",\n",
    "        \"Python is a versatile programming language.\",\n",
    "        \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ab8827-ec93-4e2f-8627-eef6fb86e162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>Bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "      <td>[Allama, Iqbal, was, a, visionary, philosopher...</td>\n",
       "      <td>[(Allama, Iqbal), (Iqbal, was), (was, a), (a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "      <td>[Thank, you, for, your, contribution, to, our,...</td>\n",
       "      <td>[(Thank, you), (you, for), (for, your), (your,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "      <td>[Python, is, a, versatile, programming, langua...</td>\n",
       "      <td>[(Python, is), (is, a), (a, versatile), (versa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "      <td>[Data, science, is, an, interdisciplinary, fie...</td>\n",
       "      <td>[(Data, science), (science, is), (is, an), (an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Allama Iqbal was a visionary philosopher and p...   \n",
       "1  Thank you for your contribution to our community.   \n",
       "2        Python is a versatile programming language.   \n",
       "3  Data science is an interdisciplinary field tha...   \n",
       "\n",
       "                                      Tokenized_Text  \\\n",
       "0  [Allama, Iqbal, was, a, visionary, philosopher...   \n",
       "1  [Thank, you, for, your, contribution, to, our,...   \n",
       "2  [Python, is, a, versatile, programming, langua...   \n",
       "3  [Data, science, is, an, interdisciplinary, fie...   \n",
       "\n",
       "                                             Bigrams  \n",
       "0  [(Allama, Iqbal), (Iqbal, was), (was, a), (a, ...  \n",
       "1  [(Thank, you), (you, for), (for, your), (your,...  \n",
       "2  [(Python, is), (is, a), (a, versatile), (versa...  \n",
       "3  [(Data, science), (science, is), (is, an), (an...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and apply bigrams to the 'Text' column\n",
    "df['Tokenized_Text'] = df['Text'].apply(word_tokenize)\n",
    "df['Bigrams'] = df['Tokenized_Text'].apply(lambda tokens: list(bigrams(tokens)))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "140aa0df-8ee1-44c7-9c08-3092ee61757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Allama Iqbal was a visionary philosopher and p...\n",
       "1  Thank you for your contribution to our community.\n",
       "2        Python is a versatile programming language.\n",
       "3  Data science is an interdisciplinary field tha..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Text': [\n",
    "        \"Allama Iqbal was a visionary philosopher and politician.\",\n",
    "        \"Thank you for your contribution to our community.\",\n",
    "        \"Python is a versatile programming language.\",\n",
    "        \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b85bf-5ffe-469a-8a80-42fbe94b7119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "268a42d0-330b-4f74-b0e6-6aea09d625bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>Bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "      <td>[Allama, Iqbal, was, a, visionary, philosopher...</td>\n",
       "      <td>[(Allama, Iqbal, was), (Iqbal, was, a), (was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "      <td>[Thank, you, for, your, contribution, to, our,...</td>\n",
       "      <td>[(Thank, you, for), (you, for, your), (for, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "      <td>[Python, is, a, versatile, programming, langua...</td>\n",
       "      <td>[(Python, is, a), (is, a, versatile), (a, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "      <td>[Data, science, is, an, interdisciplinary, fie...</td>\n",
       "      <td>[(Data, science, is), (science, is, an), (is, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Allama Iqbal was a visionary philosopher and p...   \n",
       "1  Thank you for your contribution to our community.   \n",
       "2        Python is a versatile programming language.   \n",
       "3  Data science is an interdisciplinary field tha...   \n",
       "\n",
       "                                      Tokenized_Text  \\\n",
       "0  [Allama, Iqbal, was, a, visionary, philosopher...   \n",
       "1  [Thank, you, for, your, contribution, to, our,...   \n",
       "2  [Python, is, a, versatile, programming, langua...   \n",
       "3  [Data, science, is, an, interdisciplinary, fie...   \n",
       "\n",
       "                                             Bigrams  \n",
       "0  [(Allama, Iqbal, was), (Iqbal, was, a), (was, ...  \n",
       "1  [(Thank, you, for), (you, for, your), (for, yo...  \n",
       "2  [(Python, is, a), (is, a, versatile), (a, vers...  \n",
       "3  [(Data, science, is), (science, is, an), (is, ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Tokenize and apply bigrams to the 'Text' column\n",
    "\n",
    "\n",
    "df['Tokenized_Text'] = df['Text'].apply(word_tokenize)\n",
    "df['Bigrams'] = df['Tokenized_Text'].apply(lambda tokens: list(ngrams(tokens, 3)))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94571d33",
   "metadata": {},
   "source": [
    "## h.  Stopwords Removal\n",
    "- Stopwords are extremely common words of a language having very little meanings, and it is usually safe to remove them and not consider them as important for later processing of our data.\n",
    "- Every language has its own set of stopwords. For example, some stopwords of English language are: the, a, an, was, were, at, will, on, in, from, to, me, you, yours,....\n",
    "- Whether you should remove stop words from your text or not mainly depends on the problem you are solving.\n",
    "- Remove stop words from your text if you are working on:\n",
    "    - Text Classification (Spam Filtering, Language Classification, Genre Classification)\n",
    "    - Caption Generation\n",
    "    - Auto-Tag Generation\n",
    "- Avoid removing stop words from your text if you are working on:\n",
    "    - Machine Translation\n",
    "    - Language Modeling\n",
    "    - Text Summarization\n",
    "    - Question-Answering problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ab8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c1ad24",
   "metadata": {},
   "source": [
    "### (i) Using NLTK\n",
    "- The NLTK library has a defined set of stopwords for different languages like English. Here, we will focus on ‘english’ stopwords. One can also consider additional stopwords if required\n",
    "- Note that there is no single universal list of stopwords. The list of the stop words can change depending on your problem statement\n",
    "- Once you install nltk, it just install the base library and do not install all the packages related to different languages, different tokenization schemes, etc. To install all the nltk packages and corpora use `nltk.download()`\n",
    "- An installation window will pop up. Select all and click ‘Download’ to download and install the additional bundles. This will download all the dictionaries and other language and grammar data frames necessary for full NLTK functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f5e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dbe631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80496a3",
   "metadata": {},
   "source": [
    "> After completion of downloading, you can load the package of `stopwords` from the `nltk.corpus` and use it to load the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41327c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3294b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'through', 'so', 'haven', 'am', 'than', 'needn', 'during', 'have', \"haven't\", \"shan't\", 't', 'be', 'against', 'ain', \"you'll\", 'yours', 'he', 'there', 'when', 'such', 'just', 'its', 'him', 'which', 'and', 'until', 'himself', 'over', 'if', 'were', 'yourselves', 'most', 'd', 'her', 'where', 'was', \"should've\", 'is', 'don', 'aren', 'his', 'did', 'hadn', 'having', \"weren't\", 'with', 'weren', 'they', 'in', 'very', 'been', 'from', 's', 'what', \"don't\", \"doesn't\", 'myself', 'ourselves', 'down', 'a', 'some', 'them', 'up', 'can', 'once', 'same', 'on', 'why', \"wasn't\", 'or', 'will', 'all', 'but', 'yourself', \"won't\", 'itself', 'between', 'while', 'the', 'couldn', 'for', 'mightn', 'ma', 'me', 'these', \"aren't\", 'an', \"that'll\", 'she', 'won', 'themselves', \"you've\", 'wasn', \"isn't\", 'as', 'more', 'it', 'doing', 'should', 'not', \"you'd\", 'after', \"couldn't\", 'you', 'i', 'few', 'under', 'hasn', 'how', 'has', 'y', 'no', 'do', 'wouldn', 'does', 'here', 'each', 'are', 'any', 'now', 'before', 'shan', 'had', \"you're\", 'out', 'own', 'whom', 'off', 'shouldn', 'too', 'ours', \"she's\", 'm', \"mightn't\", 'those', \"hadn't\", 'about', 'to', \"it's\", 'herself', 'again', 'by', 've', \"shouldn't\", 'll', 'above', 'being', 'this', 'my', 'that', 'because', \"needn't\", 'other', 'below', 'didn', 'our', 'isn', 'at', 'theirs', 'only', 'into', 'mustn', 'hers', 'doesn', 'then', 'nor', 'of', \"mustn't\", 'who', 're', \"hasn't\", 'both', 'further', 'we', 'your', 'o', 'their', \"wouldn't\", \"didn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2734e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39ef9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa5ee1",
   "metadata": {},
   "source": [
    "**Removing Stopwords from Text of an Email**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "813eb042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your Google account compromised. Your account closed. Immediately click link update account'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystr=\"Your Google account has been compromised. \\\n",
    "    Your account will be closed. Immediately click this link to update your account\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b91e9",
   "metadata": {},
   "source": [
    "**Removing Stopwords for a Sentiment Analysis Application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2293ec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie good'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr=\"This movie is not good\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b2be",
   "metadata": {},
   "source": [
    ">- <font color=green>For sentiment analysis purposes, the overall meaning of the resulting sentence is positive, which is not at all the reality. So either do not remove sentiment analysis while doing sentiment analysis or handle the negation before removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61521760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e374466e",
   "metadata": {},
   "source": [
    "### (ii) Using spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- **Download spacy model for English language:** Spacy comes with pretrained models and pipelines for different languages. We have already downloaded the pre-trained spacy model for English language\n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62844349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4b7db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'somehow', 'through', 'afterwards', '’ve', 'least', 'yours', 'never', 'nine', 'back', 'its', 'anyhow', 'anything', 'wherein', 'namely', 'where', 'every', 'his', 'with', 'go', 'what', 'n’t', 'top', 'them', 'first', 'same', 'herein', 'itself', 'between', 'the', 'although', 'for', 'several', 'me', 'these', 'however', 'as', 'should', 'toward', 'fifty', 'therefore', 'after', 'hereupon', 'few', 'under', 'someone', 'has', 'six', 'next', 'any', 'either', 'now', '’d', 'own', 'seeming', 'whole', 'somewhere', 'whom', \"'re\", 'front', 'thus', 'rather', '‘s', 'to', 'per', 'again', 'by', 'neither', 'hers', 'ca', 'then', 'of', 're', 'n‘t', 'everything', 'am', '‘re', 'during', 'eleven', 'seem', 'just', 'there', \"'d\", 'fifteen', 'three', 'him', 'became', 'and', 'over', 'mine', 'until', 'was', 'is', 'they', 'might', 'very', 'been', 'whereas', 'four', 'myself', 'some', 'a', 'mostly', 'say', \"'s\", 'last', 'once', 'though', 'or', 'sometimes', 'but', 'yourself', 'almost', 'none', 'five', 'seemed', 'thereafter', 'besides', 'show', 'upon', 'among', 'thru', 'doing', 'get', 'it', 'becoming', 'former', 'anyway', 'towards', 'forty', 'since', 'make', 'how', 'hereby', 'sixty', \"n't\", 'anywhere', 'eight', 'hence', 'therein', 'each', '’s', 'nothing', 'before', 'beyond', 'beforehand', 'always', 'throughout', 'whereafter', 'herself', 'give', 'perhaps', 'this', 'well', 'third', 'move', 'formerly', 'must', 'nor', 'ever', 'still', 'together', 'who', 'both', 'your', 'thence', 'so', 'keep', 'cannot', 'otherwise', 'he', 'when', 'such', 'really', 'nobody', 'were', 'us', 'amongst', 'did', 'whereupon', 'less', 'thereby', 'down', 'ourselves', 'up', 'various', 'can', 'put', 'many', 'whenever', 'will', 'all', '’m', 'anyone', 'hereafter', 'via', 'becomes', 'an', 'serious', 'whereby', '’ll', 'whether', \"'ll\", 'take', 'whose', 'you', 'seems', '‘ll', '‘ve', 'yet', 'no', 'do', 'does', 'due', 'sometime', 'whence', 'except', 'part', 'used', 'one', '‘d', 'too', 'ours', 'beside', 'those', 'indeed', 'about', 'amount', 'else', 'would', 'being', \"'ve\", 'because', 'wherever', 'other', 'below', 'our', 'only', 'into', 'nowhere', 'everyone', 'see', 'whatever', 'moreover', 'twenty', 'please', 'twelve', 'without', 'than', 'whither', 'have', 'become', 'done', 'against', 'be', 'call', 'which', 'himself', 'if', 'most', 'yourselves', 'her', 'everywhere', 'much', 'in', 'thereupon', 'from', 'unless', 'empty', 'on', 'why', \"'m\", 'quite', 'latterly', 'while', 'even', 'enough', 'alone', 'may', 'full', 'she', 'themselves', 'more', 'around', 'not', 'i', 'already', 'noone', 'something', 'here', 'are', 'ten', 'another', 'elsewhere', 'had', 'within', 'regarding', 'out', '’re', 'behind', 'off', 'latter', 'meanwhile', 'nevertheless', 'hundred', 'onto', 'made', '‘m', 'along', 'above', 'my', 'that', 'two', 'using', 'whoever', 'at', 'name', 'across', 'further', 'often', 'we', 'also', 'side', 'their', 'others', 'bottom', 'could'}\n"
     ]
    }
   ],
   "source": [
    "# returns a set of around 326 English stopwords built into spaCy\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "893c079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_spacy(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1cd4064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sample text need remove stopwords'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr=\"This is a sample text and we need to remove stopwords from it\"\n",
    "remove_stopwords_spacy(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccb9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe336f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9842b38f",
   "metadata": {},
   "source": [
    "**Add a stop word to the existing list of spaCy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d16c7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('aka')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['aka'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "778fdd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91b6366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db5e5831",
   "metadata": {},
   "source": [
    "**To remove a stop word:** Alternatively, you may decide that `'always'` should not be considered a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8f3c2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c158d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('aka')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['aka'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d008124c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07132ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f237ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc65d0-b3d4-455a-948d-ccc13ff6e14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d39b19a-4b76-4cc5-907c-dc1d65c3dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "302827bf-5844-4996-9acb-bc762a1a67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_spacy(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c753288f-9411-4ca4-8e65-26123ea47fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Allama Iqbal was a visionary philosopher and p...\n",
       "1  Thank you for your contribution to our community.\n",
       "2        Python is a versatile programming language.\n",
       "3  Data science is an interdisciplinary field tha..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Text': [\n",
    "        \"Allama Iqbal was a visionary philosopher and politician.\",\n",
    "        \"Thank you for your contribution to our community.\",\n",
    "        \"Python is a versatile programming language.\",\n",
    "        \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47b7c29c-8902-4f24-9d4c-f62ad74f5ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allama Iqbal was a visionary philosopher and p...</td>\n",
       "      <td>Allama Iqbal visionary philosopher politician.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for your contribution to our community.</td>\n",
       "      <td>Thank contribution community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a versatile programming language.</td>\n",
       "      <td>Python versatile programming language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "      <td>Data science interdisciplinary field uses scie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Allama Iqbal was a visionary philosopher and p...   \n",
       "1  Thank you for your contribution to our community.   \n",
       "2        Python is a versatile programming language.   \n",
       "3  Data science is an interdisciplinary field tha...   \n",
       "\n",
       "                              Text_without_stopwords  \n",
       "0     Allama Iqbal visionary philosopher politician.  \n",
       "1                      Thank contribution community.  \n",
       "2             Python versatile programming language.  \n",
       "3  Data science interdisciplinary field uses scie...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text_without_stopwords'] = df['Text'].apply(remove_stopwords_spacy)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf1d7d-8524-4d59-8244-e20477cc01d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d0a65-b9a1-4eb9-9448-add6f0ca7e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e787a1-ee81-404a-9835-94de634818fa",
   "metadata": {},
   "source": [
    "## 3. **Advanced Preprocessing**\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "- NER\n",
    "- Parsing\n",
    "- Coreference Resolution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759ccf7-e0f2-48bf-a006-3e211285f42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd20aac-ba25-4584-a2cc-44773ba84002",
   "metadata": {},
   "source": [
    "## (i) Stemming\n",
    "\n",
    "Stemming is a natural language processing technique used to reduce words to their base or root form. It involves the removal of prefixes, suffixes, and other variations from words to obtain a common base.\n",
    "\n",
    "### Purpose\n",
    "- **Normalization:** Stemming helps in normalizing words by converting them to a common form, reducing different inflections or derivations to a common base.\n",
    "- **Information Retrieval:** It is commonly employed in information retrieval systems to enhance search accuracy by treating different forms of a word as the same.\n",
    "\n",
    "### Process\n",
    "- **Suffix Stripping:** Stemming primarily involves stripping off suffixes from words, leaving behind the root. For example, \"running\" becomes \"run\" after stemming.\n",
    "- **Heuristic Approach:** Stemming is often rule-based and heuristic, utilizing predefined rules to identify and remove affixes. This can result in the stem being an approximation.\n",
    "\n",
    "### Example\n",
    "- Original: \"Jumps\", \"Jumping\", \"Jumped\"\n",
    "- Stemmed: \"Jump\"\n",
    "\n",
    "### Algorithms\n",
    "- **Porter Stemmer:** A widely used stemming algorithm, designed by Martin Porter, which applies a set of rules to remove suffixes.\n",
    "- **Snowball Stemmer:** An extension of the Porter Stemmer, offering more language support and customization options.\n",
    "\n",
    "### Considerations\n",
    "- **Over-stemming:** Aggressive stemming may lead to over-stemming, where different words are reduced to the same stem, potentially losing distinct meanings.\n",
    "- **Under-stemming:** Conversely, under-stemming may occur when the stemmer fails to reduce words that should share a common base.\n",
    "\n",
    "### Use Cases\n",
    "- **Search Engines:** Stemming improves search results by considering variations of words as equivalent.\n",
    "- **Information Retrieval:** Facilitates matching documents or queries by treating words in their base form.\n",
    "\n",
    "### Limitations\n",
    "- **Linguistic Ambiguity:** Stemming may not account for linguistic nuances, leading to inaccuracies.\n",
    "- **Language Specific:** Stemmers need to be designed for specific languages, as language structures vary.\n",
    "\n",
    "Stemming is a valuable tool in text processing, balancing simplicity with the need for normalized representations of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff5b80-618b-4fce-aaa1-d8bc9016f72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ab37e-2e15-4c2c-8aad-1dffa644704f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ef4c79-32eb-4ea9-92b6-224315893f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stemming', 'is', 'a', 'technique', 'used', 'for', 'reducing', 'words', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n",
      "Stemmed words: ['stem', 'is', 'a', 'techniqu', 'use', 'for', 'reduc', 'word', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Example text for stemming\n",
    "text = \"Stemming is a technique used for reducing words to their base or root form.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [porter.stem(word) for word in words]\n",
    "\n",
    "# Display the original and stemmed words\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6921a-4206-4ab6-a53a-c8d40c0d5b1f",
   "metadata": {},
   "source": [
    "> # Problem of Stemming\n",
    "techniqu is not word of English dictionary ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782aece-4217-4950-a221-063f7339eb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdcb2206-3004-44d6-845f-5b7ca22bc849",
   "metadata": {},
   "source": [
    "## Problems of Stemming\n",
    "\n",
    "### 1. **Over-stemming:**\n",
    "   - **Issue:** Aggressive stemming may result in multiple words being reduced to the same stem, leading to a loss of distinct meanings.\n",
    "   - **Example:** \"running\" and \"runner\" both reduced to \"run.\"\n",
    "\n",
    "### 2. **Under-stemming:**\n",
    "   - **Issue:** Failure to reduce related words to the same stem when they should share a common base.\n",
    "   - **Example:** \"meeting\" and \"meetings\" not reduced to the same stem.\n",
    "\n",
    "### 3. **Linguistic Ambiguity:**\n",
    "   - **Issue:** Stemming algorithms may not consider linguistic nuances, resulting in inaccuracies.\n",
    "   - **Example:** The stemmer may not differentiate between the noun \"bat\" and the verb \"bat.\"\n",
    "\n",
    "### 4. **Language Specific:**\n",
    "   - **Issue:** Stemmers need to be designed for specific languages, and their effectiveness may vary across different languages.\n",
    "   - **Example:** English-centric stemmers may not perform well on languages with different linguistic structures.\n",
    "\n",
    "### 5. **Loss of Meaning:**\n",
    "   - **Issue:** Stemming may lead to a loss of semantic meaning, as the reduced form might not accurately represent the intended sense of the word.\n",
    "   - **Example:** The stem \"sing\" might be used for both \"singing\" and \"single,\" causing confusion.\n",
    "\n",
    "### 6. **Computational Complexity:**\n",
    "   - **Issue:** Some stemming algorithms can be computationally intensive, impacting performance in large-scale text processing applications.\n",
    "   - **Example:** Applying complex rules for stemming may slow down the processing speed.\n",
    "\n",
    "### 7. **Rare or Irregular Words:**\n",
    "   - **Issue:** Stemming may not work well with rare or irregular words that do not conform to standard affix patterns.\n",
    "   - **Example:** Uncommon technical terms or proper nouns may not be stemmed correctly.\n",
    "\n",
    "### 8. **No Context Awareness:**\n",
    "   - **Issue:** Stemming algorithms typically do not consider the context of words in a sentence, which can lead to incorrect stemming decisions.\n",
    "   - **Example:** The same word may be stemmed differently based on its context in a sentence.\n",
    "\n",
    "### 9. **Dependency on Rules:**\n",
    "   - **Issue:** Many stemming algorithms rely on predefined rules, and variations outside these rules may not be handled effectively.\n",
    "   - **Example:** A novel word formation may not be stemmed correctly without specific rules for it.\n",
    "\n",
    "Understanding these problems helps in making informed decisions about when to use stemming and when alternative approaches may be more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f2239e-a728-41d3-bc13-8a9a0f5c4cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stemming is a technique.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying stemmer to DataFrame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python programming is fun!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text\n",
       "0        Stemming is a technique.\n",
       "1  Applying stemmer to DataFrame.\n",
       "2      Python programming is fun!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'text': ['Stemming is a technique.', 'Applying stemmer to DataFrame.', 'Python programming is fun!']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec30de9-7b3b-4125-996c-45e5785ed7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stemming(text):\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28cc0bd-5fd3-403c-b313-fe7e5d3d6f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stemming is a technique.</td>\n",
       "      <td>stem is a techniqu .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying stemmer to DataFrame.</td>\n",
       "      <td>appli stemmer to datafram .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python programming is fun!</td>\n",
       "      <td>python program is fun !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text                 text_stemmed\n",
       "0        Stemming is a technique.         stem is a techniqu .\n",
       "1  Applying stemmer to DataFrame.  appli stemmer to datafram .\n",
       "2      Python programming is fun!      python program is fun !"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text' + '_stemmed'] = df['text'].apply(stemming)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321acf2b-87d0-45c2-b51a-b76ad4fcf448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ca2f5c-d25e-4109-8ace-cdfaad74aa74",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a natural language processing technique that involves reducing words to their base or root form by ensuring root word belongs to the language, known as the lemma. Unlike stemming, lemmatization aims to transform words into their dictionary or grammatical form, providing a more meaningful representation.\n",
    "\n",
    "### Purpose\n",
    "- **Semantic Accuracy:** Lemmatization ensures that the resulting base form represents the actual word found in dictionaries, enhancing the semantic accuracy of the text.\n",
    "- **Context Preservation:** Unlike stemming, lemmatization considers the context of the word within the sentence, providing a more contextually appropriate base form.\n",
    "\n",
    "### Process\n",
    "- **Lemma Identification:** Lemmatization involves identifying the lemma of a word by considering its part of speech (POS) and applying linguistic rules.\n",
    "- **POS Tagging:** Prior to lemmatization, parts of speech (e.g., noun, verb) are assigned to words in the text to guide the lemmatization process.\n",
    "\n",
    "### Example\n",
    "- Original: \"Running\", \"ran\", \"runs\"\n",
    "- Lemma: \"run\"\n",
    "\n",
    "### Algorithms\n",
    "- **WordNet Lemmatizer:** Utilizes the WordNet lexical database to determine lemmas based on POS tagging.\n",
    "- **Spacy Lemmatizer:** Part of the spaCy library, which provides advanced lemmatization with support for multiple languages.\n",
    "\n",
    "### Use Cases\n",
    "- **Information Retrieval:** Lemmatization improves the accuracy of search queries by considering the base forms of words.\n",
    "- **Text Analysis:** Enhances the quality of features in natural language processing tasks such as sentiment analysis and topic modeling.\n",
    "\n",
    "### Considerations\n",
    "- **Computational Intensity:** Lemmatization can be more computationally intensive than stemming due to the need for POS tagging and dictionary lookups.\n",
    "- **Language-Specific Rules:** Lemmatization often requires language-specific rules to handle variations in morphology.\n",
    "\n",
    "### Benefits\n",
    "- **Contextual Accuracy:** Provides a more accurate base form considering the context of the word in a sentence.\n",
    "- **Improved Understanding:** Lemmatized text is often more interpretable and facilitates better understanding in linguistic analysis.\n",
    "\n",
    "Lemmatization is a powerful tool in natural language processing, offering a nuanced approach to word normalization that considers both grammatical and semantic factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df9177-1363-4cd3-a455-00e0c8b81f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabdcdd-38b3-45c5-861f-59f1b050bfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a86f2da-49b0-40f7-96b2-ec0efee91156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The cats are running and playing in the garden.\n",
      "Lemmatized Text: The cat are running and playing in the garden .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Example usage\n",
    "text_to_lemmatize = \"The cats are running and playing in the garden.\"\n",
    "lemmatized_text = lemmatize_text(text_to_lemmatize)\n",
    "print(\"Original Text:\", text_to_lemmatize)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010755f-8e3d-4e38-a354-68ae833f7743",
   "metadata": {},
   "source": [
    "> <h4>In this example, the lemmatizer has reduced \"cats\" to \"cat\" and left the other words unchanged. Keep in mind that lemmatization might not always produce perfect results, and the output may vary based on the lemmatization algorithm and the specific context of the words in the text.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf47930-f7e9-44b2-9645-c1b87d3d49ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72a57a-7e69-4e03-b5b8-452d8a6ceb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "660fd1dc-7a83-4402-9a9f-15920b617873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The dogs were barking loudly in the neighborhood.\n",
      "Lemmatized Text: The dog were barking loudly in the neighborhood .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Example usage\n",
    "text_to_lemmatize = \"The dogs were barking loudly in the neighborhood.\"\n",
    "lemmatized_text = lemmatize_text(text_to_lemmatize)\n",
    "print(\"Original Text:\", text_to_lemmatize)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2eeb2-740e-472d-99c1-67aad9d472a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd894be-d5c9-479a-93e1-a2eaec81189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cats are running and playing in the garden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dogs were barking loudly in the neighborhood.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am programming in Python and learning NLP.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0    The cats are running and playing in the garden.\n",
       "1  The dogs were barking loudly in the neighborhood.\n",
       "2       I am programming in Python and learning NLP."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Text': [\"The cats are running and playing in the garden.\",\n",
    "                 \"The dogs were barking loudly in the neighborhood.\",\n",
    "                 \"I am programming in Python and learning NLP.\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc7e0-03d8-4800-a008-be2141ce6320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded84e6f-6f5a-4ca7-824a-a42d27b936c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if the part of speech is not recognized\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e5482-e41c-4dee-9f88-0e7baf70bd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9052d12c-e51c-42c9-b270-c6bdb0faaffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verb(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=wordnet.VERB) if tag.startswith('V') else token for token, tag in tagged_tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b0fd0-0e4f-4b02-b80d-bfbd33d9e917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f46dbf-0e84-4e8d-b057-4693d5cbc216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cats are running and playing in the garden.</td>\n",
       "      <td>The cat be run and play in the garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dogs were barking loudly in the neighborhood.</td>\n",
       "      <td>The dog be bark loudly in the neighborhood .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am programming in Python and learning NLP.</td>\n",
       "      <td>I be program in Python and learn NLP .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0    The cats are running and playing in the garden.   \n",
       "1  The dogs were barking loudly in the neighborhood.   \n",
       "2       I am programming in Python and learning NLP.   \n",
       "\n",
       "                                lemmatized_text  \n",
       "0       The cat be run and play in the garden .  \n",
       "1  The dog be bark loudly in the neighborhood .  \n",
       "2        I be program in Python and learn NLP .  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized_text'] = df['Text'].apply(lemmatize_text)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e16549-2441-4783-a3c6-a7e24ab65367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed4058-9b03-4591-957a-de336ded930a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a24842-aec3-423c-9830-43a2a9d70f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4f028-9ba1-463a-9644-aaaba625997f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9706ce8",
   "metadata": {},
   "source": [
    "# 3. Text Pre-Processing on IMDB Dataset\n",
    "- Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea430cd4",
   "metadata": {},
   "source": [
    "## a. EDA on IMD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a25f6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/imdb_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e2f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a808df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6872a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     999 non-null    object\n",
      " 1   sentiment  999 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465318b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe7f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "03759f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    501\n",
       "negative    498\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the count of positive and negative reviews to ensure that the dataset is balanced\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769aecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fe2914a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(x ='sentiment', kind='count', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943778d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccc133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab4db158",
   "metadata": {},
   "source": [
    "**Reduce the records from 50K to 1K for quick processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9fdd06e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save 1000 rows in a new dataframe\n",
    "temp_df = df.iloc[0:1000,:]\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff52d83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    501\n",
       "negative    498\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the count of positive and negative reviews\n",
    "temp_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c51f4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a new csv file\n",
    "temp_df.to_csv('datasets/imdb-dataset-1000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b30712d8",
   "metadata": {},
   "source": [
    "## b. Case folding, removing digits, punctuations and substituting contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b77f60",
   "metadata": {},
   "source": [
    "**Read the Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3491317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>On watching this film, I was amazed at how med...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Nothing is sacred. Just ask Ernie Fosselius. T...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I hated it. I hate self-aware pretentious inan...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I usually try to be professional and construct...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>If you like me is going to see this in a film ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    One of the other reviewers has mentioned that ...  positive\n",
       "1    A wonderful little production. <br /><br />The...  positive\n",
       "2    I thought this was a wonderful way to spend ti...  positive\n",
       "3    Basically there's a family where a little boy ...  negative\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "994  On watching this film, I was amazed at how med...  positive\n",
       "995  Nothing is sacred. Just ask Ernie Fosselius. T...  positive\n",
       "996  I hated it. I hate self-aware pretentious inan...  negative\n",
       "997  I usually try to be professional and construct...  negative\n",
       "998  If you like me is going to see this in a film ...  negative\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./datasets/imdb-dataset-1000.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1ed2a896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967df79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452efb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d78d918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "\n",
    "def text_cleaning(mystr):\n",
    "    mystr = mystr.lower()     # case folding\n",
    "    mystr = re.sub('\\w*\\d\\w*', '', mystr) # removing digits\n",
    "    mystr = re.sub('\\n', ' ', mystr)      # replace new line characters with space\n",
    "    mystr = re.sub('[‘’“”…]', '', mystr) # removing double quotes and single quotes\n",
    "    mystr = re.sub('<.*?>', '', mystr)   # removing html tags \n",
    "    mystr = re.sub('https?://\\S+|www.\\.\\S+', '', mystr) # removing URLs\n",
    "    mystr = ''.join([c for c in mystr if c not in string.punctuation])  # remove punctuations\n",
    "    mystr = ' '.join([contractions.fix(word) for word in mystr.split()]) # expand contractions\n",
    "    return mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0137eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3d7680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>r_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there is a family where a little boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           r_cleaned  \n",
       "0  one of the other reviewers has mentioned that ...  \n",
       "1  a wonderful little production the filming tech...  \n",
       "2  i thought this was a wonderful way to spend ti...  \n",
       "3  basically there is a family where a little boy...  \n",
       "4  petter matteis love in the time of money is a ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['r_cleaned'] = df['review'].apply(text_cleaning)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f54b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8082fa37",
   "metadata": {},
   "source": [
    "## b. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d05a58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>r_cleaned</th>\n",
       "      <th>r_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there is a family where a little boy...</td>\n",
       "      <td>[basically, there, is, a, family, where, a, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           r_cleaned  \\\n",
       "0  one of the other reviewers has mentioned that ...   \n",
       "1  a wonderful little production the filming tech...   \n",
       "2  i thought this was a wonderful way to spend ti...   \n",
       "3  basically there is a family where a little boy...   \n",
       "4  petter matteis love in the time of money is a ...   \n",
       "\n",
       "                                         r_tokenized  \n",
       "0  [one, of, the, other, reviewers, has, mentione...  \n",
       "1  [a, wonderful, little, production, the, filmin...  \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
       "3  [basically, there, is, a, family, where, a, li...  \n",
       "4  [petter, matteis, love, in, the, time, of, mon...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['r_tokenized'] = df['r_cleaned'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16419956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf756416",
   "metadata": {},
   "source": [
    "## c. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "852940f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>r_cleaned</th>\n",
       "      <th>r_tokenized</th>\n",
       "      <th>r_no_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[one, reviewers, mentioned, watching, oz, epis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there is a family where a little boy...</td>\n",
       "      <td>[basically, there, is, a, family, where, a, li...</td>\n",
       "      <td>[basically, family, little, boy, jake, thinks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "      <td>[petter, matteis, love, time, money, visually,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           r_cleaned  \\\n",
       "0  one of the other reviewers has mentioned that ...   \n",
       "1  a wonderful little production the filming tech...   \n",
       "2  i thought this was a wonderful way to spend ti...   \n",
       "3  basically there is a family where a little boy...   \n",
       "4  petter matteis love in the time of money is a ...   \n",
       "\n",
       "                                         r_tokenized  \\\n",
       "0  [one, of, the, other, reviewers, has, mentione...   \n",
       "1  [a, wonderful, little, production, the, filmin...   \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
       "3  [basically, there, is, a, family, where, a, li...   \n",
       "4  [petter, matteis, love, in, the, time, of, mon...   \n",
       "\n",
       "                                             r_no_sw  \n",
       "0  [one, reviewers, mentioned, watching, oz, epis...  \n",
       "1  [wonderful, little, production, filming, techn...  \n",
       "2  [thought, wonderful, way, spend, time, hot, su...  \n",
       "3  [basically, family, little, boy, jake, thinks,...  \n",
       "4  [petter, matteis, love, time, money, visually,...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    new_words = [word for word in tokenized_text if word not in stop_words]\n",
    "    return new_words\n",
    "\n",
    "df['r_no_sw'] = df['r_tokenized'].apply(lambda token: remove_stopwords(token))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0707c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ffd3adb",
   "metadata": {},
   "source": [
    "## d. Save the Pre-Processed Dataframe in a New CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a49e3ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewers mentioned watching oz episode ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                  processed_reviews\n",
       "0  positive  one reviewers mentioned watching oz episode ho...\n",
       "1  positive  wonderful little production filming technique ...\n",
       "2  positive  thought wonderful way spend time hot summer we...\n",
       "3  negative  basically family little boy jake thinks zombie...\n",
       "4  positive  petter matteis love time money visually stunni..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the tokens of pre-processed text\n",
    "df['processed_reviews'] = df['r_no_sw'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "new_df = pd.concat([df['sentiment'], df['processed_reviews']], axis=1)\n",
    "\n",
    "# save the resulting datafrrame to a new csv file\n",
    "new_df.to_csv('datasets/processed_imdb_reviews.csv', index=False)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a63e12-b605-4004-aff5-b74d482d2a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761360b-5f5e-4be4-97a4-03995d0905fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

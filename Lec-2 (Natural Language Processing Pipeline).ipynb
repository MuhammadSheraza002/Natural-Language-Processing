{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b54689-1f23-484d-a324-a9e3125c217e",
   "metadata": {},
   "source": [
    "---   \n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Natural Language Processing</h1>\n",
    "\n",
    "--- \n",
    "<h2><div align=\"right\">Muhammad Sheraz (Data Scientist)</div></h2>\n",
    "<h1 align=\"center\">Lecture 2 (Natural Language Processing Pipeline)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703eb38-f55d-4e13-b9d9-56ae310a9d1c",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"900\"  src=\"images/nlp-overview.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b663c2-4e8b-47cc-93ae-357a1d64b646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11959bdb-0ade-41df-a1c7-59a284f01985",
   "metadata": {},
   "source": [
    "# Learning Agenda\n",
    "\n",
    "1. **NLP Pipeline**\n",
    "    - Data Acquisition\n",
    "    - Text Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Building\n",
    "    - Model Evaluation\n",
    "    - Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6a9f2-9e24-431c-8383-bc534a7f2185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66aa9f-a7b9-4428-8191-3a5904f2b4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a49511-ca2e-47c0-98ea-fb089de71555",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "<img align=\"center\" width=\"900\"  src=\"images/nlp-pipeline.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac7292-cdc3-4151-ab0a-1e3354b298af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbce48-4a47-4dc3-9fc7-9e3a5573c93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a25ff4-0a18-45bf-b967-d76b002b046d",
   "metadata": {},
   "source": [
    "# a. Data Acquisition:\n",
    "\n",
    "<img align=\"right\" width=\"600\" src=\"images/data-acquisition.png\"  >\n",
    "\n",
    "- **Use Libraries Built-in Datasets:**\n",
    "    - Seaborn: (iris, titanic, tips, flights, panguins, car_crashes)\n",
    "    - Scikit-learn: (iris, digits, diaetes, ostan housing) \n",
    "    - NLTK: (movie-reviews, product_reviews, twitter_samples, gutenerg, genesis, timeit, voice, wordnet, sentiword)\n",
    "    \n",
    "- **Use Public Dataset Repositories:**\n",
    "    - https://www.kaggle.com/\n",
    "    - https://data.gov/\n",
    "    - https://archive.ics.uci.edu/ml/index.php\n",
    "    - https://github.com/\n",
    "- **Use Company's Database:** (SQL, NOSQL, Data warehouse, Data lake)\n",
    "- **Generate your own Datasets:**\n",
    "    - Use Web scraping or Web API </li>\n",
    "    - IoT Devices </li>\n",
    "    - Crowd Sourcing (Amazon Mechanical Turk, Lionbridge AI)\n",
    "    - Data Augmentation\n",
    "    \n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Better data often beats better algorithms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ead93-95c7-4b2d-a40e-9b52e9492980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de7ea4-a14d-42b6-8c54-257edf647523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1913ced3-7dec-4c4b-b517-3d069d729b56",
   "metadata": {},
   "source": [
    "# b. Text Pre-Processing\n",
    "\n",
    "<img align=\"right\" width=\"600\" src=\"images/text-preprocessing.png\"  >\n",
    "\n",
    "- **Text Cleaning:** \n",
    "    - Removing digits and words containing digits\n",
    "    - Removing newline characters and extra spaces\n",
    "    - Removing HTML tags\n",
    "    - Removing URLs\n",
    "    - Removing punctuations\n",
    "    - Handle emojis\n",
    "    - Spelling correction\n",
    "\n",
    "- **Basic Preprocessing:** \n",
    "    - Case folding\n",
    "    - Expand contractions\n",
    "    - Chat word treatment\n",
    "    - Spelling correction\n",
    "    - Tokenization and N-grams\n",
    "    - Removing stop words\n",
    "\n",
    "- **Advance Preprocessing:**\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - POS tagging\n",
    "    - Parsing\n",
    "    - Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14fd17-8fce-4d61-96a4-8913c3526acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca0c99e8-1c66-4115-88aa-e1a58eca6d27",
   "metadata": {},
   "source": [
    "# c. Feature Engineering\n",
    "<img align=\"center\" width=\"900\"  src=\"images/feature-engr-models.png\"  > \n",
    "\n",
    "- Once the text pre-processing is done, we need to transform it into their features to be used for modeling. We assign numeric weights to words within our document, so that it can be fed to some machine/deep learning algorithm. This assignment of weights to words is done in such a way that the number represent the meaning of that word.\n",
    "\n",
    "- The process of converting text data into vectors of real numbers is called `Feature Extraction from text` or `Text Representation` or `Text Vectorization`, whose goal is converting the text data into numbers in such a way that those numbers should be able to tell the semantic or meaning of those words.\n",
    "\n",
    "- The two main categories and their sub-categories of word embeddings are:\n",
    "    - **Frequency Based Word Embedding Techniques:**\n",
    "        - Bag of Words (BoW)\n",
    "        - Term Frequency - Inverse Document Frequency (TFIDF)\n",
    "        - Global Vectors (GloVe)\n",
    "    - **Prediction Based Word Embedding Techniques:**\n",
    "        - Word2Vec (Google, 2013)\n",
    "        -  FastText (Facebook, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eeaf8-eff3-404d-8b36-59b930ca3d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b2867-8e3b-412e-98c7-30681c44a3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffd44c04-2373-4c16-8613-39ba13c32929",
   "metadata": {},
   "source": [
    "# d. Model Building\n",
    "\n",
    "<img align=\"center\" width=\"800\"  src=\"images/dl-approach.png\"  > \n",
    "\n",
    "- The machine learning approaches can be supervised or unsupervised and can be classification or regression\n",
    "    - Supervised ML Algorithms used in NLP:\n",
    "        - Naïve Bayes\n",
    "        - Logistic Regression\n",
    "        - Support Vector Machine\n",
    "        - Decision Trees\n",
    "        - Random Forrest\n",
    "    - Un-supervised ML Algorithms used in NLP:\n",
    "        - K-Mean Clustering\n",
    "        - Latent Semantic Indexing\n",
    "        - Latent Dirichlet allocation (LDA)\n",
    "        - Non-negative Matrix Factorization\n",
    "        - Hidden Markov Model (HMMs can be trained both in an unsupervised and in a supervised fashion)\n",
    "\n",
    "\n",
    "- Deep Learning models\n",
    "    - CNN (Convolutional Neural Networks)\n",
    "    - RNN (Recurrent Neural Networks)\n",
    "    - LSTM (Long-term Short-Term Memory)\n",
    "    - GRU (Gated Recurrent Unit)\n",
    "    - Transformers\n",
    "        - BERT (Bidirectional Encoder Representations from Transformers)\n",
    "        - GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "- If the dataset has a fewer number of observations and a higher number of features, choose algorithms with high bias/low variance like Linear regression, Naïve Bayes, Linear SVM\n",
    "- If the training data is sufficiently large and the number of observations is higher as compared to the number of features, one can go for low bias/high variance algorithms like KNN, Decision trees, kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72439d9-2b79-4a65-a121-e4c6e89610e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fceca20-6fb5-4541-b919-0678cce81a45",
   "metadata": {},
   "source": [
    "# e. Model Evaluation\n",
    "\n",
    "<img align=\"center\" width=\"500\"  src=\"images/model-evaluation.jpeg\"  > \n",
    "\n",
    "- Model evaluation is the process of using different evaluation metrics to understand a machine learning model's performance, as well as its strengths and weaknesses\n",
    "- **Evaluation Metrics for Classification Algorithms:**\n",
    "    - Confusion Matrix\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1-Score\n",
    "    - AUC-ROC (Receiver Operator Characteristic (ROC) curve\n",
    "- **Evaluation Metrics for Regression Algorithms:**\n",
    "    - Mean Absolute Error (MAE)\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "    - R-Squared (coefficient of determination)\n",
    "    - Adjusted R-Squared\n",
    ">- For details read this blog: https://blog.knoldus.com/model-evaluation-metrics-for-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e820e2-5627-4080-b549-655aa7ff9752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e917e989-bb0d-4c85-84da-39c33f466f47",
   "metadata": {},
   "source": [
    "# f. Model Deployment\n",
    "\n",
    "<img align=\"center\" width=\"800\"  src=\"images/deployment.png\"  > \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b8853-bbd0-4c7b-8803-7b5bffac8aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
